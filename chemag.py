from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import os
import json
import base64
from io import BytesIO

# HJ ChemAgent ê´€ë ¨ ì„í¬íŠ¸
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import tool
from typing import Annotated
from langchain_core.messages import BaseMessage, ToolMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
import requests
import urllib.parse

# ë…¼ë¬¸ ê²€ìƒ‰ ê´€ë ¨ ì„í¬íŠ¸
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup, Comment
import asyncio
import re
from urllib.parse import urlparse, urljoin
import nest_asyncio
nest_asyncio.apply()
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# Scimago Journal Rank ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì„í¬íŠ¸
import sqlite3
from datetime import datetime, timedelta
from dataclasses import dataclass
import logging
from functools import lru_cache

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="HJ ChemAgent Simple")

# LLM ì„¤ì •
os.environ["GOOGLE_API_KEY"] = 'AIzaSyAPf9KX4Ea_MKiC_tYfm8OUlv_A71BUz1I'
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-001",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    system_instruction="""ë‹¹ì‹ ì€ í™”í•™ ì—°êµ¬ ë³´ì¡° AIì…ë‹ˆë‹¤. 
    
    ğŸš¨ CRITICAL RULE - HTML ì™„ì „ ì¶œë ¥:
    ë„êµ¬ê°€ HTMLì„ ë°˜í™˜í•˜ë©´ (íŠ¹íˆ <div>, <table> íƒœê·¸):
    1. HTMLì„ ì™„ì „íˆ, ëê¹Œì§€, ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš” - ì ˆëŒ€ë¡œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ì§€ ë§ˆì„¸ìš”
    2. HTMLì„ ìë¥´ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ˆì„¸ìš” - ì „ì²´ HTML ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥
    3. ì ˆëŒ€ ì¶”ê°€ ì„¤ëª… ê¸ˆì§€: "ê²°ê³¼ì…ë‹ˆë‹¤", "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤", "ë¶„ì„í–ˆìŠµë‹ˆë‹¤" ë“±
    4. HTML ì•ë’¤ì— ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ë¶™ì´ì§€ ë§ˆì„¸ìš”
    5. ë„êµ¬ê°€ ì™„ì „í•œ HTML í…Œì´ë¸”ì„ ë°˜í™˜í–ˆë‹¤ë©´ ê·¸ ì „ì²´ë¥¼ ì¶œë ¥í•˜ì„¸ìš”
    6. ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”(|---|---|---)ë¡œ ë³€í™˜í•˜ì§€ ë§ˆì„¸ìš” - HTML ê·¸ëŒ€ë¡œ ì¶œë ¥
    
    ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
    ë„êµ¬ ì¶œë ¥: "<div class="research-papers-table enhanced-search"><h3>ğŸ”¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼...</h3><table>...</table></div>"
    â†’ ë‹¹ì‹ ì˜ ì‘ë‹µ: <div class="research-papers-table enhanced-search"><h3>ğŸ”¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼...</h3><table>...</table></div>
    
    ì ˆëŒ€ ê¸ˆì§€:
    â†’ "doxorubicinì— ëŒ€í•œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. | ë…¼ë¬¸ ì œëª© | ì£¼ìš” ë‚´ìš©"
    â†’ HTMLì„ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ëŠ” í–‰ìœ„
    â†’ HTMLì„ ì¤‘ê°„ì— ìë¥´ëŠ” í–‰ìœ„
    â†’ HTML ì•ë’¤ì— ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ê°€
    
    HTMLì´ ì•„ë‹Œ ì¼ë°˜ ì§ˆë¬¸ì€ í‰ì†Œì²˜ëŸ¼ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.""",
)

# Scimago Journal Rank ë°ì´í„° í´ë˜ìŠ¤
@dataclass
class JournalMetrics:
    """ì €ë„ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    journal_name: str
    sjr_score: float
    h_index: int
    total_docs: int
    total_cites: int
    cites_per_doc: float
    country: str
    publisher: str
    categories: list
    quartile: str
    rank_in_category: int
    percentile: float
    issn: str
    last_updated: str

# Scimago Journal Rank ë¶„ì„ê¸° (ê°„ì†Œí™”ëœ ë²„ì „)
class ScimagoJournalAnalyzer:
    """Scimago Journal Rank ë°ì´í„°ë¥¼ í™œìš©í•œ ì €ë„ ë¶„ì„ê¸° (ChemAgent í†µí•©ìš©)"""
    
    def __init__(self, cache_db_path: str = "journal_cache.db"):
        self.base_url = "https://www.scimagojr.com"
        self.cache_db_path = cache_db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_cache_db()
        
        # ì‹¤ì‹œê°„ Scimago ê²€ìƒ‰ì„ ìœ„í•œ ì„¤ì •
        self.scimago_search_url = "https://www.scimagojr.com/journalsearch.php"
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def _init_cache_db(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.cache_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS journal_metrics (
                    journal_name_key TEXT PRIMARY KEY,
                    journal_name TEXT,
                    sjr_score REAL,
                    h_index INTEGER,
                    quartile TEXT,
                    last_updated TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("Scimago ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Scimago ìºì‹œ DB ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")

    def _clean_journal_name_for_search(self, journal_name: str) -> list:
        """ê²€ìƒ‰ìš© ì €ë„ëª… ì •ë¦¬ (ì—¬ëŸ¬ ë³€í˜• ë°˜í™˜)"""
        variants = []
        
        # ì›ë³¸
        variants.append(journal_name)
        
        # ê´„í˜¸ì™€ ë‚´ìš© ì œê±°
        cleaned = re.sub(r'\s*\([^)]*\)', '', journal_name).strip()
        if cleaned != journal_name:
            variants.append(cleaned)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        simple = re.sub(r'[^\w\s]', '', journal_name).strip()
        if simple != journal_name and simple not in variants:
            variants.append(simple)
        
        # ì²« ë‹¨ì–´ë§Œ
        first_word = journal_name.split()[0]
        if len(first_word) > 3 and first_word not in variants:
            variants.append(first_word)
        
        return variants

    def search_scimago_realtime(self, journal_name: str) -> dict:
        """Scimagoì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ë„ ë©”íŠ¸ë¦­ ê²€ìƒ‰ (ì •í™•í•œ XPath ì‚¬ìš©)"""
        try:
            print(f"[SCIMAGO] ì‹¤ì‹œê°„ ê²€ìƒ‰: {journal_name}")
            
            # ì €ë„ëª… ë³€í˜•ë“¤ ìƒì„±
            search_variants = self._clean_journal_name_for_search(journal_name)
            print(f"[SCIMAGO] ê²€ìƒ‰ ë³€í˜•ë“¤: {search_variants}")
            
            # ê° ë³€í˜•ì— ëŒ€í•´ ê²€ìƒ‰ ì‹œë„
            for variant in search_variants:
                print(f"[SCIMAGO] ë³€í˜• '{variant}'ë¡œ ê²€ìƒ‰ ì‹œë„")
                
                # 1ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ì €ë„ ë§í¬ ì°¾ê¸°
                search_params = {
                    'q': variant.replace(' ', '+')
                }
                
                search_url = f"{self.scimago_search_url}?q={variant.replace(' ', '+')}"
                print(f"[SCIMAGO] ê²€ìƒ‰ URL: {search_url}")
                
                search_response = self.session.get(self.scimago_search_url, params=search_params, timeout=15)
                print(f"[SCIMAGO] ê²€ìƒ‰ ì‘ë‹µ ìƒíƒœ: {search_response.status_code}")
                
                if search_response.status_code == 200:
                    search_soup = BeautifulSoup(search_response.content, 'html.parser')
                    print(f"[SCIMAGO] HTML ê¸¸ì´: {len(search_response.content)} bytes")
                    
                    # XPath: /html/body/div[4]/div[2]/a ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œ ì°¾ê¸°
                    journal_links = search_soup.select('div:nth-of-type(4) div:nth-of-type(2) a')
                    print(f"[SCIMAGO] XPath ë§í¬ ê°œìˆ˜: {len(journal_links)}")
                    
                    if not journal_links:
                        # ëŒ€ì•ˆ: ì €ë„ëª…ì„ í¬í•¨í•œ ëª¨ë“  ë§í¬ ì°¾ê¸°
                        all_links = search_soup.find_all('a', href=True)
                        journal_links = [link for link in all_links if 'journalsearch.php?q=' in link.get('href', '')]
                        print(f"[SCIMAGO] ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì°¾ì€ ë§í¬ ê°œìˆ˜: {len(journal_links)}")
                        
                        # ì²˜ìŒ 3ê°œ ë§í¬ ì •ë³´ ì¶œë ¥
                        for i, link in enumerate(journal_links[:3]):
                            print(f"[SCIMAGO] ë§í¬ {i+1}: {link.get('href')} - {link.get_text()[:50]}")
                    
                    if journal_links:
                        # ì²« ë²ˆì§¸ ë§¤ì¹­ ì €ë„ ì„ íƒ
                        first_link = journal_links[0]
                        journal_href = first_link.get('href')
                        link_text = first_link.get_text().strip()
                        
                        print(f"[SCIMAGO] ì„ íƒëœ ë§í¬: {journal_href}")
                        print(f"[SCIMAGO] ë§í¬ í…ìŠ¤íŠ¸: {link_text}")
                        
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if journal_href.startswith('journalsearch.php'):
                            journal_url = f"https://www.scimagojr.com/{journal_href}"
                        else:
                            journal_url = journal_href
                        
                        print(f"[SCIMAGO] ì €ë„ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™: {journal_url}")
                        
                        # 2ë‹¨ê³„: ì €ë„ ìƒì„¸ í˜ì´ì§€ì—ì„œ SJR ì ìˆ˜ ì¶”ì¶œ
                        detail_response = self.session.get(journal_url, timeout=15)
                        print(f"[SCIMAGO] ìƒì„¸ í˜ì´ì§€ ì‘ë‹µ ìƒíƒœ: {detail_response.status_code}")
                        
                        if detail_response.status_code == 200:
                            detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                            print(f"[SCIMAGO] ìƒì„¸ í˜ì´ì§€ HTML ê¸¸ì´: {len(detail_response.content)} bytes")
                            
                            # XPathë¡œ SJR ì ìˆ˜ ì¶”ì¶œ
                            metrics = self._extract_scimago_metrics_from_detail(detail_soup, journal_name)
                            
                            if metrics and metrics.get('sjr_score', 0) > 0:
                                self._cache_scimago_metrics(journal_name, metrics)
                                print(f"[SCIMAGO] ê²€ìƒ‰ ì„±ê³µ (ë³€í˜•: {variant}): {journal_name} â†’ SJR: {metrics['sjr_score']:.2f}")
                                return metrics
                            else:
                                print(f"[SCIMAGO] SJR ì¶”ì¶œ ì‹¤íŒ¨ (ë³€í˜•: {variant}): {journal_name}")
                    else:
                        print(f"[SCIMAGO] ì €ë„ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë³€í˜•: {variant})")
                else:
                    print(f"[SCIMAGO] ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨ (ë³€í˜•: {variant}): {search_response.status_code}")
            
            print(f"[SCIMAGO] ëª¨ë“  ë³€í˜• ê²€ìƒ‰ ì‹¤íŒ¨: {journal_name}")
            return None
            
        except Exception as e:
            logger.error(f"Scimago ì‹¤ì‹œê°„ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            print(f"[SCIMAGO] ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
            import traceback
            print(f"[SCIMAGO] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return None
    
    def _extract_scimago_metrics_from_detail(self, soup, journal_name: str) -> dict:
        """ì €ë„ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ XPathë¡œ SJRê³¼ Quartile ì¶”ì¶œ"""
        try:
            print(f"[SCIMAGO] ìƒì„¸ í˜ì´ì§€ì—ì„œ SJRê³¼ Quartile ì¶”ì¶œ ì¤‘...")
            
            # XPath: /html/body/div[7]/div/div/div[4]/p[1] ì—ì„œ SJRê³¼ Quartile ì¶”ì¶œ
            # ì˜ˆì‹œ: <p class="hindexnumber">0.562 <span class="Q3" style="font-size: 0.8em" title="best quartile for all subject categories">Q3</span></p>
            target_elements = soup.select('body > div:nth-of-type(7) > div > div > div:nth-of-type(4) > p:nth-of-type(1)')
            
            sjr_score = 0.0
            quartile = "Unknown"
            h_index = 0
            
            # SJR ì ìˆ˜ì™€ Quartile ì¶”ì¶œ
            if target_elements:
                target_element = target_elements[0]
                element_text = target_element.get_text(strip=True)
                print(f"[SCIMAGO] ëŒ€ìƒ ìš”ì†Œ í…ìŠ¤íŠ¸: {element_text}")
                
                # SJR ì ìˆ˜ ì¶”ì¶œ (ì²« ë²ˆì§¸ ìˆ«ì)
                sjr_numbers = re.findall(r'\d+\.\d+', element_text)
                if sjr_numbers:
                    sjr_score = float(sjr_numbers[0])
                    print(f"[SCIMAGO] SJR ì ìˆ˜ ì¶”ì¶œ: {sjr_score}")
                
                # Quartile ì¶”ì¶œ (Q1, Q2, Q3, Q4)
                quartile_spans = target_element.find_all('span', class_=re.compile(r'^Q[1-4]$'))
                if quartile_spans:
                    quartile = quartile_spans[0].get_text(strip=True)
                    print(f"[SCIMAGO] Quartile ì¶”ì¶œ: {quartile}")
                else:
                    # span íƒœê·¸ê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ
                    quartile_match = re.search(r'Q[1-4]', element_text)
                    if quartile_match:
                        quartile = quartile_match.group()
                        print(f"[SCIMAGO] Quartile í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ: {quartile}")
            
            # ëŒ€ì•ˆ: ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œ SJR ì°¾ê¸°
            if sjr_score == 0:
                # class="hindexnumber" ìš”ì†Œ ì°¾ê¸°
                hinumber_elements = soup.find_all('p', class_='hindexnumber')
                for elem in hinumber_elements:
                    elem_text = elem.get_text(strip=True)
                    sjr_numbers = re.findall(r'\d+\.\d+', elem_text)
                    if sjr_numbers:
                        sjr_score = float(sjr_numbers[0])
                        print(f"[SCIMAGO] hindexnumberì—ì„œ SJR ì¶”ì¶œ: {sjr_score}")
                        
                        # ê°™ì€ ìš”ì†Œì—ì„œ Quartileë„ ì°¾ê¸°
                        q_spans = elem.find_all('span', class_=re.compile(r'^Q[1-4]$'))
                        if q_spans:
                            quartile = q_spans[0].get_text(strip=True)
                            print(f"[SCIMAGO] hindexnumberì—ì„œ Quartile ì¶”ì¶œ: {quartile}")
                        break
            
            # H-index ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            h_elements = soup.find_all(string=re.compile(r'H index', re.IGNORECASE))
            for h_elem in h_elements:
                parent = h_elem.parent
                if parent:
                    h_text = parent.get_text()
                    h_numbers = re.findall(r'\d+', h_text)
                    if h_numbers:
                        h_index = int(h_numbers[0])
                        print(f"[SCIMAGO] H-index ì¶”ì¶œ: {h_index}")
                        break
            
            if sjr_score > 0:
                # IFëŠ” SJRì—ì„œ ì¶”ì •
                impact_factor_estimate = sjr_score * 2.5
                
                return {
                    "journal": journal_name,
                    "sjr_score": sjr_score,
                    "h_index": h_index,
                    "quartile": quartile,
                    "impact_factor_estimate": impact_factor_estimate,
                    "source": "scimago_realtime_detail"
                }
            
            print(f"[SCIMAGO] SJR ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        except Exception as e:
            logger.error(f"Scimago SJR/Quartile ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            print(f"[SCIMAGO] SJR/Quartile ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _extract_scimago_metrics(self, soup, journal_name: str) -> dict:
        """Scimago í˜ì´ì§€ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (êµ¬ë²„ì „ - ë°±ì—…ìš©)"""
        try:
            # êµ¬ë²„ì „ ë¡œì§ ìœ ì§€ (ë°±ì—…ìš©) - ê²½ê³  ìˆ˜ì •
            sjr_elements = soup.find_all(text=re.compile(r'\d+\.\d+'))  # ì¼ë‹¨ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            sjr_score = 0.0
            h_index = 0
            
            for element in sjr_elements[:3]:
                try:
                    value = float(element.strip())
                    if 0.1 <= value <= 50:  # SJR ì ìˆ˜ ë²”ìœ„
                        sjr_score = value
                        break
                except:
                    continue
            
            h_elements = soup.find_all(text=re.compile(r'H index'))  # ì¼ë‹¨ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            if h_elements:
                for h_elem in h_elements:
                    parent = h_elem.parent
                    if parent:
                        numbers = re.findall(r'\d+', parent.get_text())
                        if numbers:
                            h_index = int(numbers[0])
                            break
            
            if sjr_score > 0:
                return {
                    "journal": journal_name,
                    "sjr_score": sjr_score,
                    "h_index": h_index,
                    "quartile": self._estimate_quartile(sjr_score),
                    "impact_factor_estimate": sjr_score * 2.5,
                    "source": "scimago_realtime"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Scimago ë©”íŠ¸ë¦­ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _estimate_quartile(self, sjr_score: float) -> str:
        """SJR ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Quartile ì¶”ì •"""
        if sjr_score >= 10:
            return "Q1"
        elif sjr_score >= 3:
            return "Q1"
        elif sjr_score >= 1:
            return "Q2"
        elif sjr_score >= 0.3:
            return "Q3"
        else:
            return "Q4"
    
    def _cache_scimago_metrics(self, journal_name: str, metrics: dict):
        """Scimago ë©”íŠ¸ë¦­ì„ ìºì‹œì— ì €ì¥"""
        try:
            journal_key = journal_name.lower().strip()
            conn = sqlite3.connect(self.cache_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO journal_metrics VALUES 
                (?, ?, ?, ?, ?, ?)
            ''', (
                journal_key,
                metrics['journal'],
                metrics['sjr_score'],
                metrics['h_index'],
                metrics['quartile'],
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Scimago ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    def get_journal_metrics_fast(self, journal_name: str) -> dict:
        """ì €ë„ ë©”íŠ¸ë¦­ ì¡°íšŒ (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ ê²€ìƒ‰)"""
        try:
            journal_key = journal_name.lower().strip()
            print(f"[SCIMAGO] ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹œì‘: {journal_name} (í‚¤: {journal_key})")
            
            # 1ë‹¨ê³„: ìºì‹œì—ì„œ ì¡°íšŒ
            cached_metrics = self._get_cached_metrics(journal_key)
            if cached_metrics and cached_metrics.get('sjr_score', 0) > 0:
                print(f"[SCIMAGO] ìºì‹œì—ì„œ ì¡°íšŒ ì„±ê³µ: {journal_name} â†’ SJR: {cached_metrics.get('sjr_score', 0)}")
                return cached_metrics
            else:
                print(f"[SCIMAGO] ìºì‹œì— ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ: {journal_name}")
            
            # 2ë‹¨ê³„: ì‹¤ì‹œê°„ Scimago ê²€ìƒ‰
            print(f"[SCIMAGO] ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œì‘: {journal_name}")
            realtime_metrics = self.search_scimago_realtime(journal_name)
            if realtime_metrics and realtime_metrics.get('sjr_score', 0) > 0:
                print(f"[SCIMAGO] ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ê³µ: {journal_name}")
                return realtime_metrics
            else:
                print(f"[SCIMAGO] ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹¤íŒ¨: {journal_name}")
            
            # 3ë‹¨ê³„: ê¸°ë³¸ê°’ ë°˜í™˜
            print(f"[SCIMAGO] ê¸°ë³¸ê°’ ë°˜í™˜: {journal_name}")
            return {
                "journal": journal_name,
                "sjr_score": 0.0,
                "h_index": 0,
                "quartile": "Unknown",
                "impact_factor_estimate": 0.0,
                "source": "unknown"
            }
            
        except Exception as e:
            logger.error(f"ì €ë„ ë©”íŠ¸ë¦­ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            print(f"[SCIMAGO] ì˜¤ë¥˜ ë°œìƒ: {journal_name} - {str(e)}")
            return {
                "journal": journal_name,
                "sjr_score": 0.0,
                "h_index": 0,
                "quartile": "Unknown",
                "impact_factor_estimate": 0.0,
                "source": "error"
            }

    def _get_cached_metrics(self, journal_key: str) -> dict:
        """ìºì‹œì—ì„œ ì €ë„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.cache_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT journal_name, sjr_score, h_index, quartile, last_updated
                FROM journal_metrics 
                WHERE journal_name_key = ? AND last_updated > ?
            ''', (journal_key, (datetime.now() - timedelta(days=30)).isoformat()))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                sjr_score = result[1] if result[1] is not None else 0.0
                h_index = result[2] if result[2] is not None else 0
                quartile = result[3] if result[3] is not None else "Unknown"
                
                print(f"[SCIMAGO] ìºì‹œ ë°ì´í„° ë°œê²¬: {result[0]} â†’ SJR: {sjr_score}, Q: {quartile}")
                
                if sjr_score > 0:  # ìœ íš¨í•œ SJR ì ìˆ˜ê°€ ìˆì„ ë•Œë§Œ ë°˜í™˜
                    return {
                        "journal": result[0],
                        "sjr_score": sjr_score,
                        "h_index": h_index,
                        "quartile": quartile,
                        "impact_factor_estimate": sjr_score * 2.5,
                        "source": "cached"
                    }
                else:
                    print(f"[SCIMAGO] ìºì‹œ ë°ì´í„° ë¬´íš¨ (SJR=0): {result[0]}")
            else:
                print(f"[SCIMAGO] ìºì‹œì— ë°ì´í„° ì—†ìŒ: {journal_key}")
            
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            print(f"[SCIMAGO] ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return None

    def calculate_impact_score(self, journal_metrics: dict, citation_count: int = 0) -> float:
        """ì¢…í•© ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°"""
        try:
            sjr_score = journal_metrics.get('sjr_score', 0)
            h_index = journal_metrics.get('h_index', 0)
            
            # SJR ì ìˆ˜ ê°€ì¤‘ì¹˜ (40%)
            sjr_component = sjr_score * 4.0
            
            # H-index ê°€ì¤‘ì¹˜ (30%)
            h_index_component = (h_index / 100) * 30
            
            # ì¸ìš©ìˆ˜ ê°€ì¤‘ì¹˜ (30%)
            citation_component = min(citation_count / 50, 1.0) * 30
            
            total_score = sjr_component + h_index_component + citation_component
            
            return round(total_score, 2)
            
        except Exception as e:
            logger.error(f"ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.0

    def get_impact_tier(self, impact_score: float) -> tuple:
        """ì˜í–¥ë ¥ ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶„ë¥˜"""
        if impact_score >= 50:
            return ("ğŸ”¥ Top Tier", "high-impact")
        elif impact_score >= 30:
            return ("â­ High Impact", "medium-high-impact")
        elif impact_score >= 15:
            return ("ğŸ“ˆ Notable", "medium-impact")
        elif impact_score >= 5:
            return ("ğŸ“„ Standard", "standard-impact")
        else:
            return ("ğŸ“ Emerging", "low-impact")

# ì „ì—­ Scimago ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
scimago_analyzer = ScimagoJournalAnalyzer()

# ë…¼ë¬¸ ê²€ìƒ‰ í´ë˜ìŠ¤ ì •ì˜ (ì˜ ì‘ë™í•˜ëŠ” ì›ë³¸ ë²„ì „)
class paper_search:
    def __init__(self, max_articles=5):
        self.max_articles = max_articles
        self.base_url = "https://pubmed.ncbi.nlm.nih.gov/"
        self.playwright = None
        self.browser = None
        self.context = None

    async def setup_browser(self):
        if self.playwright is None:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--disable-gpu'
                ]
            )
            self.context = await self.browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )

    async def close_browser(self):
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        self.playwright = None
        self.browser = None
        self.context = None

    def clean_title(self, title: str) -> str:
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^\W+|\W+$', '', title)
        return title if len(title) > 10 else ""

    async def extract_articles(self, page, query: str) -> list:
        articles = []
        print(f"ë…¼ë¬¸ ì¶”ì¶œ ì‹œì‘... ìµœëŒ€ {self.max_articles}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤.")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        try:
            await page.wait_for_selector('#search-results', timeout=10000)
            print("ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ ë¡œë”© ì™„ë£Œ")
        except:
            print("ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜¬ë°”ë¥¸ PubMed selectorë“¤
        selectors_to_try = [
            'a.docsum-title',  # ì‹¤ì œ êµ¬ì¡°: <a class="docsum-title" href="...">
            '.docsum-title',   # í´ë˜ìŠ¤ë§Œìœ¼ë¡œë„ ê°€ëŠ¥
            '#search-results a.docsum-title',  # ë” êµ¬ì²´ì ì¸ ê²½ë¡œ
            'a[data-ga-category="result_click"]',  # ë°ì´í„° ì†ì„± í™œìš©
            'a[data-article-id]',  # ë…¼ë¬¸ ID ì†ì„±ì´ ìˆëŠ” ë§í¬
            '[data-ga-action]'  # GA ì•¡ì…˜ ì†ì„±ì´ ìˆëŠ” ìš”ì†Œ
        ]
        
        # ëª¨ë“  ë…¼ë¬¸ ë§í¬ë¥¼ í•œ ë²ˆì— ì°¾ê¸°
        all_links = []
        for selector in selectors_to_try:
            try:
                links = await page.query_selector_all(selector)
                if links:
                    print(f"'{selector}'ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                    all_links = links
                    break
            except Exception as e:
                print(f"Selector '{selector}' ì˜¤ë¥˜: {str(e)}")
                continue
        
        if not all_links:
            print("ì–´ë–¤ selectorë¡œë„ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # HTML êµ¬ì¡° ë””ë²„ê¹…
            try:
                html_content = await page.inner_html('#search-results')
                print("ê²€ìƒ‰ ê²°ê³¼ HTML ì¼ë¶€:")
                print(html_content[:500] + "...")
            except:
                print("HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì°¾ì€ ë§í¬ë“¤ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        for i in range(min(len(all_links), self.max_articles)):
            try:
                link = all_links[i]
                print(f"ë…¼ë¬¸ {i+1} ì¶”ì¶œ ì¤‘...")
                
                title = await link.inner_text()
                url = await link.get_attribute('href')
                
                print(f"ë…¼ë¬¸ {i+1} ì œëª©: {title[:50]}...")
                print(f"ë…¼ë¬¸ {i+1} URL: {url}")
                
                # URL ì •ê·œí™”
                if url and not url.startswith("http"):
                    url = urljoin(self.base_url, url)
                
                title = self.clean_title(title)
                if title and url:
                    articles.append({"title": title, "url": url})
                    print(f"ë…¼ë¬¸ {i+1}: ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œë¨")
                else:
                    print(f"ë…¼ë¬¸ {i+1}: ì œëª© ë˜ëŠ” URLì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                    
            except Exception as e:
                print(f"ë…¼ë¬¸ {i+1} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
                
        print(f"ì´ {len(articles)}ê°œì˜ ë…¼ë¬¸ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return articles

    async def extract_abstract(self, url: str) -> str:
        """ë…¼ë¬¸ í˜ì´ì§€ì—ì„œ ì´ˆë¡ì„ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        page = await self.context.new_page()
        try:
            print(f"ì´ˆë¡ ì¶”ì¶œì„ ìœ„í•´ í˜ì´ì§€ ì ‘ì†: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=20000)
            await asyncio.sleep(3)
            
            # ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì´ˆë¡ ì°¾ê¸°
            abstract_text = ""
            
            # ë°©ë²• 1: #abstract .abstract-content ì‚¬ìš©
            abstract_content = await page.query_selector('#abstract .abstract-content')
            if abstract_content:
                abstract_text = await abstract_content.inner_text()
                print("ë°©ë²• 1ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
            else:
                # ë°©ë²• 2: #eng-abstract ì‚¬ìš© (ì˜ì–´ ì´ˆë¡)
                eng_abstract = await page.query_selector('#eng-abstract')
                if eng_abstract:
                    abstract_text = await eng_abstract.inner_text()
                    print("ë°©ë²• 2ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
                else:
                    # ë°©ë²• 3: #abstract ì „ì²´ ì‚¬ìš©
                    abstract_element = await page.query_selector('#abstract')
                    if abstract_element:
                        abstract_text = await abstract_element.inner_text()
                        # "Abstract" ì œëª© ì œê±°
                        abstract_text = re.sub(r'^Abstract\s*', '', abstract_text.strip())
                        print("ë°©ë²• 3ìœ¼ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print("ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ì´ˆë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return "ì´ˆë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return abstract_text.strip() if abstract_text else "ì´ˆë¡ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"ì´ˆë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ì´ˆë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        finally:
            await page.close()

    async def run_pipeline(self, query: str) -> list:
        await self.setup_browser()
        search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query}&filter=datesearch.y_5&filter=simsearch1.fha&filter=pubt.clinicaltrial"
        
        print(f"ê²€ìƒ‰ URL: {search_url}")
        
        # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        articles = []
        
        for attempt in range(max_retries):
            page = await self.context.new_page()
            try:
                print(f"ì‹œë„ {attempt + 1}/{max_retries}")
                
                # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                await asyncio.sleep(2 * (attempt + 1))  # ì ì§„ì  ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
                
                print("ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                
                # í˜ì´ì§€ ì œëª© í™•ì¸
                page_title = await page.title()
                print(f"í˜ì´ì§€ ì œëª©: {page_title}")
                
                # 403 ì˜¤ë¥˜ ê°ì§€
                if "403" in page_title or "Forbidden" in page_title:
                    print(f"403 ì˜¤ë¥˜ ê°ì§€ë¨. ì‹œë„ {attempt + 1}")
                    if attempt < max_retries - 1:
                        await page.close()
                        continue
                    else:
                        print("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                        await page.close()
                        break
                
                # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                articles = await self.extract_articles(page, query)
                await page.close()
                break
                
            except Exception as e:
                print(f"ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                await page.close()
                if attempt < max_retries - 1:
                    continue
                else:
                    print("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                    break

        results = []
        print(f"\nì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
        
        for i, article in enumerate(articles, 1):
            print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
            # ì´ˆë¡ ì¶”ì¶œì„ ìœ„í•´ ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
            abstract = await self.extract_abstract(article["url"])
            results.append({
                "title": article["title"],
                "url": article["url"],
                "abstract": abstract
            })

        await self.close_browser()
        return results

# ì „ì—­ ë…¼ë¬¸ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤
paper_searcher = paper_search(max_articles=5)

# í™”í•™ ë„êµ¬ë“¤ ì •ì˜
@tool
def name_to_smiles_cir(name: str) -> str:
    """IUPAC Nameì„ SMILESë¡œ ë³€í™˜"""
    encoded_name = urllib.parse.quote(name)
    url = f"https://cactus.nci.nih.gov/chemical/structure/{encoded_name}/smiles"
    
    try:
        response = requests.get(url, timeout=10)
        smiles = response.text.strip()
        return smiles
    except Exception as e:
        return f"Error converting name to SMILES: {str(e)}"

@tool
def generate_molecule_image(smiles: str) -> str:
    """Generate and return base64 encoded image of a molecule from its SMILES string."""
    try:
        # RDKit ì„í¬íŠ¸ ì‹œë„
        try:
            from rdkit import Chem
            from rdkit.Chem import Draw
        except ImportError:
            return "RDKitì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ RDKitì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:\n\nconda install -c conda-forge rdkit\n\në˜ëŠ”\n\npip install rdkit"
        
        # SMILESë¥¼ ë¶„ìë¡œ ë³€í™˜
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return "Invalid SMILES string."
        
        # ë¶„ì ì´ë¯¸ì§€ ìƒì„±
        image = Draw.MolToImage(mol, size=(300, 300))
        
        # PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        return f"data:image/png;base64,{img_base64}"
        
    except Exception as e:
        return f"Error generating molecule image: {str(e)}"

def search_research_papers_trends_sync(query: str, max_papers: int = 10) -> str:
    """ì—°êµ¬ ë™í–¥ ë¶„ì„ì„ ìœ„í•œ ë” ë„“ì€ ë²”ìœ„ì˜ ë…¼ë¬¸ ê²€ìƒ‰ (í•„í„° ì™„í™”)"""
    try:
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        import asyncio
        
        async def _search_papers_for_trends():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨ (ë™í–¥ ë¶„ì„ìš©)")
            
            # í•„í„°ë¥¼ ì™„í™”í•œ ê²€ìƒ‰ URL (ëª¨ë“  ë…¼ë¬¸ íƒ€ì… í¬í•¨, ìµœê·¼ 5ë…„)
            await searcher.setup_browser()
            search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query}&filter=datesearch.y_5"
            
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ê²€ìƒ‰ URL: {search_url}")
            
            # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            articles = []
            
            for attempt in range(max_retries):
                page = await searcher.context.new_page()
                try:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}/{max_retries}")
                    
                    # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                    await asyncio.sleep(2 * (attempt + 1))
                    
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                    await asyncio.sleep(5)
                    
                    print("[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                    
                    # í˜ì´ì§€ ì œëª© í™•ì¸
                    page_title = await page.title()
                    print(f"[DEBUG] í˜ì´ì§€ ì œëª©: {page_title}")
                    
                    # 403 ì˜¤ë¥˜ ê°ì§€
                    if "403" in page_title or "Forbidden" in page_title:
                        print(f"[DEBUG] 403 ì˜¤ë¥˜ ê°ì§€ë¨. ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}")
                        if attempt < max_retries - 1:
                            await page.close()
                            continue
                        else:
                            print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                            await page.close()
                            break
                    
                    # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                    articles = await searcher.extract_articles(page, query)
                    await page.close()
                    break
                    
                except Exception as e:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    await page.close()
                    if attempt < max_retries - 1:
                        continue
                    else:
                        print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                        break

            results = []
            print(f"\n[DEBUG] ë™í–¥ ë¶„ì„ìš© ì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
            
            for i, article in enumerate(articles, 1):
                print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
                abstract = await searcher.extract_abstract(article["url"])
                results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "abstract": abstract
                })

            await searcher.close_browser()
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers_for_trends())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers_for_trends())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            results = asyncio.run(_search_papers_for_trends())
        
        if not results:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            return f"'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # ê¹”ë”í•œ HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = """
<div class="research-papers-table">
<h3>ğŸ“š ì—°êµ¬ ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 5%;">ë²ˆí˜¸</th>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì´ˆë¡ ìš”ì•½</th>
<th style="width: 10%;">ë§í¬</th>
</tr>
</thead>
<tbody>
""".format(query)
        
        for i, paper in enumerate(results, 1):
            # ì´ˆë¡ì„ 200ìë¡œ ì œí•œ
            abstract_summary = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
            
            html_table += f"""
<tr>
<td>{i}</td>
<td class="paper-title">{paper['title']}</td>
<td class="paper-abstract">{abstract_summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ë³´ê¸°</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
</div>
"""
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        return html_table
        
    except Exception as e:
        error_msg = f"ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def search_research_papers_sync(query: str, max_papers: int = 5) -> str:
    """í™”í•™ ì—°êµ¬ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë™ê¸° ë²„ì „)"""
    try:
        print(f"[DEBUG] ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        import asyncio
        
        async def _search_papers():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨")
            results = await searcher.run_pipeline(query)
            print(f"[DEBUG] ê²€ìƒ‰ ê²°ê³¼: {len(results) if results else 0}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                import concurrent.futures
                import threading
                
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)  # íƒ€ì„ì•„ì›ƒì„ 120ì´ˆë¡œ ì¦ê°€
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            results = asyncio.run(_search_papers())
        
        if not results:
            print(f"[DEBUG] ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            # ë” ìƒì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š '{query}' ê²€ìƒ‰ ê²°ê³¼</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:<br><br>
â€¢ ë” ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš© (ì˜ˆ: "cancer treatment" ëŒ€ì‹  "cancer")<br>
â€¢ ì˜ì–´ ê²€ìƒ‰ì–´ ì‚¬ìš©<br>
â€¢ ë‹¤ë¥¸ í™”í•™ ë¬¼ì§ˆëª… ì‹œë„<br>
â€¢ ê²€ìƒ‰ì–´ë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ë³€ê²½
</p>
</div>
"""
        
        print(f"[DEBUG] HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # ê¹”ë”í•œ HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = """
<div class="research-papers-table">
<h3>ğŸ“š ì—°êµ¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 5%;">ë²ˆí˜¸</th>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì´ˆë¡ ìš”ì•½</th>
<th style="width: 10%;">ë§í¬</th>
</tr>
</thead>
<tbody>
""".format(query)
        
        for i, paper in enumerate(results, 1):
            # ì´ˆë¡ì„ 200ìë¡œ ì œí•œ
            abstract_summary = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
            
            html_table += f"""
<tr>
<td>{i}</td>
<td class="paper-title">{paper['title']}</td>
<td class="paper-abstract">{abstract_summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ë³´ê¸°</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
</div>
"""
        
        print(f"[DEBUG] HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        return html_table
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
</p>
</div>
"""

@tool
def paper_search_tool(query: str) -> str:
    """ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ì €ë„ IF ì ìˆ˜(SJR ê¸°ë°˜) ë¶„ì„ê³¼ í•¨ê»˜ HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë…¼ë¬¸ ê²€ìƒ‰ì„ ìš”ì²­í•˜ë©´ ìë™ìœ¼ë¡œ ì €ë„ ì˜í–¥ë ¥ë„ ë¶„ì„í•©ë‹ˆë‹¤. ë°˜í™˜ëœ HTMLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”."""
    try:
        print(f"[TOOL] ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì‹œì‘: {query}")
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        import asyncio
        
        async def _search_pipeline():
            pipeline = paper_search(max_articles=5)
            results = await pipeline.run_pipeline(query)
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_pipeline())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                results = loop.run_until_complete(_search_pipeline())
        except RuntimeError:
            results = asyncio.run(_search_pipeline())
        
        if not results:
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> "APIë¡œ {query} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¼ê³  ìš”ì²­í•˜ë©´ ë” ì•ˆì •ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</p>
</div>
"""
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì„ ìš”ì•½í•˜ê³  í‘œë¡œ ì •ë¦¬
        print(f"[TOOL] LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½ ì‹œì‘...")
        
        # ëª¨ë“  ë…¼ë¬¸ ì •ë³´ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ìš”ì•½ ìš”ì²­
        papers_info = ""
        for i, paper in enumerate(results, 1):
            papers_info += f"""
ë…¼ë¬¸ {i}:
ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}
URL: {paper['url']}

---
"""
        
        summary_prompt = f"""
ë‹¤ìŒ {len(results)}ê°œì˜ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ê° ë…¼ë¬¸ë§ˆë‹¤ 2-3ì¤„ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.

{papers_info}

ê° ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ë…¼ë¬¸1||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
ë…¼ë¬¸2||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
...

ì£¼ìš”ë‚´ìš©ìš”ì•½ì€ ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ 2-3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ìš”ì•½ ìˆ˜í–‰
        summary_result = llm.invoke(summary_prompt)
        summary_lines = summary_result.content.strip().split('\n')
        
        # ì €ë„ë³„ SJR ì ìˆ˜ ë¶„ì„ ì¶”ê°€
        print(f"[TOOL] ì €ë„ SJR ì ìˆ˜ ë¶„ì„ ì¤‘...")
        enhanced_results = []
        
        for paper in results:
            # ë…¼ë¬¸ ì œëª©ì—ì„œ ì €ë„ëª… ì¶”ì • (ê°œì„ ëœ ë§¤ì¹­ ë¡œì§)
            estimated_journal = "Unknown"
            best_match_metrics = None
            
            # Playwright ê²€ìƒ‰ì—ì„œëŠ” ì‹¤ì œ ì €ë„ëª…ì„ ì–»ê¸° ì–´ë ¤ìš°ë¯€ë¡œ 
            # ë…¼ë¬¸ ì œëª© ê¸°ë°˜ ì¶”ì •ì€ ë¶€ì •í™•í•˜ì—¬ ì œê±°
            # ëŒ€ì‹  ì‹¤ì œ ì €ë„ëª…ì´ ìˆëŠ” API ê²€ìƒ‰ì„ ìš°ì„  ì‚¬ìš©í•˜ë„ë¡ ì•ˆë‚´
            print(f"[TOOL] Playwright ê²€ìƒ‰ì€ ì €ë„ëª… ì¶”ì •ì´ ì–´ë ¤ì›€. API ê²€ìƒ‰ ê¶Œì¥.")
            
            # ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
            impact_score = 0.0
            impact_tier = "ğŸ“ Unknown"
            impact_class = "unknown-impact"
            
            if best_match_metrics:
                impact_score = scimago_analyzer.calculate_impact_score(best_match_metrics, 0)
                impact_tier, impact_class = scimago_analyzer.get_impact_tier(impact_score)
            
            enhanced_results.append({
                'paper': paper,
                'journal': estimated_journal,
                'metrics': best_match_metrics,
                'impact_score': impact_score,
                'impact_tier': impact_tier,
                'impact_class': impact_class
            })
        
        # ì˜í–¥ë ¥ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        enhanced_results.sort(key=lambda x: x['impact_score'], reverse=True)
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„± (SJR í¬í•¨)
        html_table = f"""
<div class="research-papers-table enhanced-search">
<h3>ğŸ”¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ (IF ì ìˆ˜ í¬í•¨): {query}</h3>
<div class="search-summary" style="background-color: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
    <p style="color: #0369a1; margin: 0;">
    ğŸ“Š <strong>Enhanced Search:</strong> Scimago Journal Rank (SJR) ê¸°ë°˜ IF ì ìˆ˜ ë¶„ì„ í¬í•¨<br>
    ğŸ¯ ì˜í–¥ë ¥ ì ìˆ˜ìˆœ ì •ë ¬ | ğŸ“ˆ ì €ë„ í’ˆì§ˆ í‰ê°€ | ğŸ“š IF ì ìˆ˜ ìë™ ê³„ì‚°
    </p>
</div>
<table class="papers-table enhanced-table" style="width: 100%; table-layout: fixed; min-width: 1200px;">
<thead>
<tr>
<th style="width: 40%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 20%;">ì¶”ì • ì €ë„ (SJR)</th>
<th style="width: 35%;">ì£¼ìš” ë‚´ìš© (AI ìš”ì•½)</th>
<th style="width: 5%;">ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # í–¥ìƒëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í…Œì´ë¸” í–‰ìœ¼ë¡œ ì¶”ê°€
        for i, enhanced_result in enumerate(enhanced_results):
            paper = enhanced_result['paper']
            journal = enhanced_result['journal']
            metrics = enhanced_result['metrics']
            impact_score = enhanced_result['impact_score']
            impact_tier = enhanced_result['impact_tier']
            impact_class = enhanced_result['impact_class']
            
            # LLM ìš”ì•½ì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì˜ ìš”ì•½ ì°¾ê¸°
            paper_summary = "ìš”ì•½ ìƒì„± ì¤‘..."
            for line in summary_lines:
                if f"ë…¼ë¬¸{i+1}||" in line:
                    parts = line.split("||")
                    if len(parts) >= 3:
                        paper_summary = parts[2].strip()
                    break
            
            # ìš”ì•½ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆë¡ ì‚¬ìš©
            if paper_summary == "ìš”ì•½ ìƒì„± ì¤‘..." or not paper_summary:
                paper_summary = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
            
            # ì €ë„ ì •ë³´ êµ¬ì„±
            if metrics and metrics.get('sjr_score', 0) > 0:
                journal_info = f"""{journal}<br>
                <small style='color: #666;'>
                SJR: {metrics['sjr_score']:.2f} ({metrics['quartile']})
                </small>"""
            else:
                journal_info = f"""{journal}<br>
                <small style='color: #999;'>SJR ì •ë³´ ì—†ìŒ</small>"""
            
            # ì˜í–¥ë ¥ ë°°ì§€
            impact_badge = f"""
            <div class="impact-badge {impact_class}" style="text-align: center; padding: 0.4rem; border-radius: 0.4rem; border: 2px solid; font-size: 0.8em;">
                <div style="font-weight: bold;">{impact_score:.1f}</div>
                <div style="font-size: 0.7em;">{impact_tier.split()[0]}</div>
            </div>
            """
            
            html_table += f"""
<tr class="enhanced-paper-row">
<td class="paper-title" style="word-wrap: break-word; padding: 0.8rem; line-height: 1.3;"><strong>{paper['title']}</strong></td>
<td class="journal-info" style="word-wrap: break-word; padding: 0.8rem;">{journal_info}</td>
<td class="paper-summary" style="word-wrap: break-word; padding: 0.8rem; line-height: 1.4;">{paper_summary}</td>
<td style="padding: 0.5rem;"><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸</a></td>
</tr>
"""
        
        html_table += f"""
</tbody>
</table>

</div>

<style>
.enhanced-search .impact-badge {{
    font-size: 0.8em;
}}
.enhanced-search .high-impact {{ background: #fef2f2; border-color: #ef4444; color: #dc2626; }}
.enhanced-search .medium-high-impact {{ background: #fff7ed; border-color: #f97316; color: #ea580c; }}
.enhanced-search .medium-impact {{ background: #fefce8; border-color: #eab308; color: #ca8a04; }}
.enhanced-search .standard-impact {{ background: #f0fdf4; border-color: #22c55e; color: #16a34a; }}
.enhanced-search .low-impact {{ background: #f8fafc; border-color: #64748b; color: #475569; }}
.enhanced-search .unknown-impact {{ background: #f1f5f9; border-color: #94a3b8; color: #64748b; }}

.enhanced-search .impact-legend {{
    padding: 0.25rem 0.5rem;
    border-radius: 0.25rem;
    border: 1px solid;
    font-size: 0.75em;
    white-space: nowrap;
}}

.enhanced-search .enhanced-paper-row:hover {{
    background-color: #f9fafb;
}}

.enhanced-search .journal-info {{
    font-size: 0.85em;
}}

.enhanced-search .enhanced-table {{
    min-width: 1200px;
}}

.enhanced-search .paper-title {{
    font-size: 0.95em;
    line-height: 1.3;
}}

.enhanced-search .paper-summary {{
    font-size: 0.9em;
    line-height: 1.4;
}}
</style>
"""
        
        print(f"[TOOL] ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì™„ë£Œ: {len(results)}ê°œ ë…¼ë¬¸")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜: {str(e)}"
        print(f"[TOOL] ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ğŸ’¡ <strong>ëŒ€ì•ˆ:</strong> "APIë¡œ {query} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.
</p>
</div>
"""

# í™”í•™ ë…¼ë¬¸ ë¶„ì„ ë„êµ¬
@tool
def analyze_chemical_paper(paper_url: str) -> str:
    """PubMed ë…¼ë¬¸ URLì„ ë°›ì•„ì„œ í™”í•™ì  ê´€ì ì—ì„œ ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì‹œì‘: {paper_url}")
        
        # ë…¼ë¬¸ ë‚´ìš© ì¶”ì¶œ
        import asyncio
        
        async def _extract_paper_content():
            searcher = paper_search(max_articles=1)
            await searcher.setup_browser()
            
            try:
                # ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì´ˆë¡ê³¼ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                abstract = await searcher.extract_abstract(paper_url)
                
                # ì¶”ê°€ë¡œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                page = await searcher.context.new_page()
                await page.goto(paper_url, wait_until='domcontentloaded', timeout=20000)
                await asyncio.sleep(3)
                
                # ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
                title_element = await page.query_selector('h1.heading-title, .abstract-title, h1')
                title = await title_element.inner_text() if title_element else "ì œëª© ì—†ìŒ"
                
                # ì €ì ì •ë³´ ì¶”ì¶œ
                authors_elements = await page.query_selector_all('.authors .authors-list-item, .contrib-author')
                authors = []
                for author_elem in authors_elements[:5]:  # ìµœëŒ€ 5ëª…
                    try:
                        author_name = await author_elem.inner_text()
                        authors.append(author_name.strip())
                    except:
                        continue
                
                # ë°œí–‰ ì •ë³´ ì¶”ì¶œ
                pub_date_element = await page.query_selector('.cit, .publication-date, .pub-date')
                pub_date = await pub_date_element.inner_text() if pub_date_element else "ë‚ ì§œ ì—†ìŒ"
                
                await page.close()
                await searcher.close_browser()
                
                return {
                    "title": title,
                    "authors": authors,
                    "publication_date": pub_date,
                    "abstract": abstract
                }
                
            except Exception as e:
                await searcher.close_browser()
                raise e
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_extract_paper_content())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    paper_data = future.result(timeout=60)
            else:
                paper_data = loop.run_until_complete(_extract_paper_content())
        except RuntimeError:
            paper_data = asyncio.run(_extract_paper_content())
        
        # LLMì„ ì‚¬ìš©í•œ í™”í•™ì  ë¶„ì„
        analysis_prompt = f"""
ë‹¤ìŒ í™”í•™/ì˜í•™ ë…¼ë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {paper_data['title']}
ì €ì: {', '.join(paper_data['authors'][:3])}{'...' if len(paper_data['authors']) > 3 else ''}
ë°œí–‰ì¼: {paper_data['publication_date']}

ì´ˆë¡:
{paper_data['abstract']}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì—°êµ¬ëœ í™”í•™ ë¬¼ì§ˆ ë˜ëŠ” ì•½ë¬¼
2. ì£¼ìš” ì—°êµ¬ ë°©ë²•ë¡ 
3. í™”í•™ì /ì•½ë¦¬í•™ì  ë©”ì»¤ë‹ˆì¦˜
4. ì£¼ìš” ë°œê²¬ì‚¬í•­
5. ì„ìƒì  ì˜ì˜
6. í•œê³„ì  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥

ë¶„ì„ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ë¶„ì„ ìˆ˜í–‰
        analysis_result = llm.invoke(analysis_prompt)
        
        # HTML í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì •ë¦¬
        html_result = f"""
<div class="paper-analysis">
<h3>ğŸ“‹ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼</h3>
<div style="background-color: #f8f9fa; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
<h4 style="color: #1f2937; margin-bottom: 0.5rem;">ë…¼ë¬¸ ì •ë³´</h4>
<p><strong>ì œëª©:</strong> {paper_data['title']}</p>
<p><strong>ì €ì:</strong> {', '.join(paper_data['authors'][:3])}{'...' if len(paper_data['authors']) > 3 else ''}</p>
<p><strong>ë°œí–‰ì¼:</strong> {paper_data['publication_date']}</p>
<p><strong>ë§í¬:</strong> <a href="{paper_url}" target="_blank" style="color: #10b981;">ì›ë¬¸ ë³´ê¸°</a></p>
</div>

<div style="background-color: #ffffff; padding: 1rem; border: 1px solid #e5e7eb; border-radius: 0.5rem;">
<h4 style="color: #1f2937; margin-bottom: 1rem;">ğŸ”¬ í™”í•™ì  ë¶„ì„</h4>
<div style="white-space: pre-wrap; line-height: 1.6;">{analysis_result.content}</div>
</div>
</div>
"""
        
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì™„ë£Œ")
        return html_result
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì˜¤ë¥˜: {error_msg}")
        return f"""
<div class="paper-analysis">
<h3>ğŸ“‹ ë…¼ë¬¸ ë¶„ì„ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ì˜¬ë°”ë¥¸ PubMed URLì¸ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
</p>
</div>
"""

@tool
def analyze_chemical_research_trends(topic: str, max_papers: int = 10) -> str:
    """í™”í•™ ì£¼ì œì˜ ì—°êµ¬ ë™í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë…¼ë¬¸ ê²€ìƒ‰ê³¼ í•¨ê»˜ ê° ë…¼ë¬¸ì˜ ì €ë„ IF ì ìˆ˜(SJR ê¸°ë°˜)ì™€ ì˜í–¥ë ¥ ë¶„ì„ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ 'ì—°êµ¬ ë™í–¥', 'ë…¼ë¬¸ ë™í–¥', 'IF ì ìˆ˜', 'SJR ë¶„ì„' ë“±ì„ ìš”ì²­í•˜ë©´ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì™„ì „í•œ HTML í‘œë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”."""
    try:
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë¶„ì„ ì‹œì‘: {topic}")
        
        # ë…¼ë¬¸ ê²€ìƒ‰ ìˆ˜í–‰
        import asyncio
        
        async def _search_papers_for_trends():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨ (ë™í–¥ ë¶„ì„ìš©)")
            
            # í•„í„°ë¥¼ ì™„í™”í•œ ê²€ìƒ‰ URL (ëª¨ë“  ë…¼ë¬¸ íƒ€ì… í¬í•¨, ìµœê·¼ 5ë…„)
            await searcher.setup_browser()
            search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={topic}&filter=datesearch.y_5"
            
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ê²€ìƒ‰ URL: {search_url}")
            
            # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            articles = []
            
            for attempt in range(max_retries):
                page = await searcher.context.new_page()
                try:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}/{max_retries}")
                    
                    # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                    await asyncio.sleep(2 * (attempt + 1))
                    
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                    await asyncio.sleep(5)
                    
                    print("[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                    
                    # í˜ì´ì§€ ì œëª© í™•ì¸
                    page_title = await page.title()
                    print(f"[DEBUG] í˜ì´ì§€ ì œëª©: {page_title}")
                    
                    # 403 ì˜¤ë¥˜ ê°ì§€
                    if "403" in page_title or "Forbidden" in page_title:
                        print(f"[DEBUG] 403 ì˜¤ë¥˜ ê°ì§€ë¨. ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}")
                        if attempt < max_retries - 1:
                            await page.close()
                            continue
                        else:
                            print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                            await page.close()
                            break
                    
                    # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                    articles = await searcher.extract_articles(page, topic)
                    await page.close()
                    break
                    
                except Exception as e:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    await page.close()
                    if attempt < max_retries - 1:
                        continue
                    else:
                        print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                        break

            results = []
            print(f"\n[DEBUG] ë™í–¥ ë¶„ì„ìš© ì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
            
            for i, article in enumerate(articles, 1):
                print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
                abstract = await searcher.extract_abstract(article["url"])
                results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "abstract": abstract
                })

            await searcher.close_browser()
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers_for_trends())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers_for_trends())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            results = asyncio.run(_search_papers_for_trends())
        
        if not results:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            return f"""
<div class="research-trends">
<h3>ğŸ“Š '{topic}' ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ì¶©ë¶„í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë™í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> ë” ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš” (ì˜ˆ: "cancer treatment", "drug delivery" ë“±)
</p>
</div>
"""
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½ ìƒì„±
        papers_summaries = []
        for i, paper in enumerate(results, 1):
            print(f"[DEBUG] ë…¼ë¬¸ {i} ìš”ì•½ ìƒì„± ì¤‘...")
            paper_prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ 2-3ì¤„ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}

ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
            try:
                paper_summary_result = llm.invoke(paper_prompt)
                paper_summary = paper_summary_result.content.strip()
                
                # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(paper_summary) > 200:
                    paper_summary = paper_summary[:200] + "..."
                    
                papers_summaries.append(paper_summary)
                    
            except Exception as e:
                print(f"[DEBUG] ë…¼ë¬¸ {i} ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                fallback_summary = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
                papers_summaries.append(fallback_summary)
        
        # ì—°êµ¬ ë™í–¥ ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ìš”ì•½ ìƒì„±
        all_papers_info = ""
        for i, (paper, summary) in enumerate(zip(results, papers_summaries), 1):
            all_papers_info += f"""
ë…¼ë¬¸ {i}: {paper['title']}
ì£¼ìš” ë‚´ìš©: {summary}

"""
        
        trend_analysis_prompt = f"""
ë‹¤ìŒì€ '{topic}' ì£¼ì œë¡œ ê²€ìƒ‰ëœ {len(results)}ê°œì˜ ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤:

{all_papers_info}

ì´ ë…¼ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ì£¼ìš” ì—°êµ¬ ë™í–¥ (2-3ê°œì˜ í•µì‹¬ íŠ¸ë Œë“œ)
2. ì£¼ëª©í•  ë§Œí•œ ì—°êµ¬ ì„±ê³¼
3. í–¥í›„ ì—°êµ¬ ë°©í–¥ ì „ë§

ê° í•­ëª©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        try:
            trend_analysis_result = llm.invoke(trend_analysis_prompt)
            trend_analysis = trend_analysis_result.content.strip()
        except Exception as e:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            trend_analysis = "ë™í–¥ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ì €ë„ë³„ SJR ì ìˆ˜ ë¶„ì„
        print(f"[DEBUG] ì €ë„ SJR ì ìˆ˜ ë¶„ì„ ì¤‘...")
        journal_metrics_map = {}
        unique_journals = []
        
        # ë…¼ë¬¸ì—ì„œ ì €ë„ëª… ì¶”ì¶œ (Playwright ê²€ìƒ‰ ê²°ê³¼ìš©)
        # Playwright ê²€ìƒ‰ì—ì„œëŠ” ì €ë„ ì •ë³´ë¥¼ ì§ì ‘ ì–»ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ì œëª© ê¸°ë°˜ ì¶”ì • ì‚¬ìš©
        for paper in results:
            # ë…¼ë¬¸ ì œëª©ì—ì„œ ì €ë„ëª… í‚¤ì›Œë“œ ì°¾ê¸°
            paper_title_lower = paper.get('title', '').lower()
            found_journal = None
            
            # Playwright ê²€ìƒ‰ì—ì„œëŠ” ì •í™•í•œ ì €ë„ëª… ì¶”ì¶œì´ ì–´ë ¤ìš°ë¯€ë¡œ
            # ì œëª© ê¸°ë°˜ ì¶”ì •ì€ ë¶€ì •í™•í•˜ì—¬ ì œê±°
            # ì—°êµ¬ ë™í–¥ ë¶„ì„ì—ì„œëŠ” ì „ì²´ì ì¸ íŠ¸ë Œë“œì— ì§‘ì¤‘í•˜ê³ 
            # ê°œë³„ ë…¼ë¬¸ì˜ ì •í™•í•œ IFëŠ” API ê²€ìƒ‰ì„ ê¶Œì¥
            
            if found_journal and found_journal not in unique_journals:
                unique_journals.append(found_journal)
                print(f"[DEBUG] ì €ë„ ì¶”ì •: {paper_title_lower[:50]}... â†’ {found_journal}")
        
        # ê° ì €ë„ì˜ SJR ì ìˆ˜ ì¡°íšŒ
        for journal in unique_journals[:10]:  # ìµœëŒ€ 10ê°œ ì €ë„ë§Œ ì²˜ë¦¬
            try:
                metrics = scimago_analyzer.get_journal_metrics_fast(journal)
                if metrics and metrics.get('sjr_score', 0) > 0:
                    journal_metrics_map[journal] = metrics
            except Exception as e:
                print(f"[DEBUG] ì €ë„ ë¶„ì„ ì˜¤ë¥˜ ({journal}): {str(e)}")
                continue
        
        # ë…¼ë¬¸ê³¼ ì €ë„ ë©”íŠ¸ë¦­ì„ ê²°í•©í•˜ì—¬ ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
        enhanced_papers = []
        for i, (paper, summary) in enumerate(zip(results, papers_summaries), 1):
            # ë…¼ë¬¸ì—ì„œ ì €ë„ëª… ì¶”ì • (ê°œì„ ëœ ë§¤ì¹­ ë¡œì§)
            estimated_journal = "Unknown"
            best_match_metrics = None
            
            # ì•Œë ¤ì§„ ì €ë„ê³¼ ë§¤ì¹­ ì‹œë„ (ë” ì •êµí•œ ë°©ë²•)
            paper_title_lower = paper['title'].lower()
            best_match_score = 0
            
            # ì œëª© ê¸°ë°˜ ì €ë„ ì¶”ì •ì€ ë¶€ì •í™•í•˜ë¯€ë¡œ ì œê±°
            # ì—°êµ¬ ë™í–¥ ë¶„ì„ì—ì„œëŠ” ë…¼ë¬¸ ìˆ˜ì™€ í‚¤ì›Œë“œ ë¶„ì„ì— ì§‘ì¤‘
            print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë¶„ì„: ì €ë„ë³„ IF ë¶„ì„ë³´ë‹¤ëŠ” ì „ì²´ íŠ¸ë Œë“œì— ì§‘ì¤‘")
            
            # ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
            impact_score = 0.0
            impact_tier = "ğŸ“ Unknown"
            impact_class = "unknown-impact"
            
            if best_match_metrics:
                impact_score = scimago_analyzer.calculate_impact_score(best_match_metrics, 0)
                impact_tier, impact_class = scimago_analyzer.get_impact_tier(impact_score)
            
            enhanced_papers.append({
                'paper': paper,
                'summary': summary,
                'journal': estimated_journal,
                'metrics': best_match_metrics,
                'impact_score': impact_score,
                'impact_tier': impact_tier,
                'impact_class': impact_class
            })
        
        # ì˜í–¥ë ¥ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        enhanced_papers.sort(key=lambda x: x['impact_score'], reverse=True)
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„± (SJR ì ìˆ˜ í¬í•¨)
        html_table = f"""
<div class="research-trends enhanced-trends">
<h3>ğŸ“Š '{topic}' ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ (SJR ë¶„ì„ í¬í•¨)</h3>
<div class="trend-summary" style="background-color: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
    <p style="color: #0369a1; margin: 0;">
    ğŸ”¬ <strong>Enhanced Analysis:</strong> Scimago Journal Rank (SJR) ì˜í–¥ë ¥ ë¶„ì„ í¬í•¨<br>
    ğŸ“ˆ ì˜í–¥ë ¥ ì ìˆ˜ìˆœ ì •ë ¬ | ğŸ“š ì €ë„ í’ˆì§ˆ í‰ê°€ | ğŸ¯ ì—°êµ¬ ë™í–¥ ë¶„ì„
    </p>
</div>
<table class="papers-table enhanced-trends-table" style="margin-top: 1rem;">
<thead>
<tr>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 25%;">ì¶”ì • ì €ë„ (SJR)</th>
<th style="width: 35%;">ì£¼ìš” ë‚´ìš©</th>
<th style="width: 5%;">ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # ê° ë…¼ë¬¸ ì •ë³´ë¥¼ í…Œì´ë¸” í–‰ìœ¼ë¡œ ì¶”ê°€ (SJR ì •ë³´ í¬í•¨)
        for enhanced_paper in enhanced_papers:
            paper = enhanced_paper['paper']
            summary = enhanced_paper['summary']
            journal = enhanced_paper['journal']
            metrics = enhanced_paper['metrics']
            impact_score = enhanced_paper['impact_score']
            impact_tier = enhanced_paper['impact_tier']
            impact_class = enhanced_paper['impact_class']
            
            # ì €ë„ ì •ë³´ êµ¬ì„±
            if metrics and metrics.get('sjr_score', 0) > 0:
                journal_info = f"""{journal}<br>
                <small style='color: #666;'>
                SJR: {metrics['sjr_score']:.2f} ({metrics['quartile']})
                </small>"""
            else:
                journal_info = f"""{journal}<br>
                <small style='color: #999;'>SJR ì •ë³´ ì—†ìŒ</small>"""
            
            # ì˜í–¥ë ¥ ë°°ì§€
            impact_badge = f"""
            <div class="impact-badge {impact_class}" style="text-align: center; padding: 0.5rem; border-radius: 0.5rem; border: 2px solid;">
                <div style="font-weight: bold; font-size: 1em;">{impact_score:.1f}</div>
                <div style="font-size: 0.7em;">{impact_tier}</div>
            </div>
            """
            
            html_table += f"""
<tr class="enhanced-trend-row">
<td class="paper-title"><strong>{paper['title']}</strong></td>
<td class="journal-info">{journal_info}</td>
<td class="paper-summary">{summary}</td>
<td class="impact-score">{impact_badge}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸</a></td>
</tr>
"""
        
        html_table += f"""
</tbody>
</table>

<div style="margin-top: 2rem; padding: 1.5rem; background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 0.5rem;">
<h4 style="color: #1f2937; margin-bottom: 1rem;">ğŸ“ˆ ì—°êµ¬ ë™í–¥ ë¶„ì„</h4>
<div style="white-space: pre-wrap; line-height: 1.6; color: #374151;">{trend_analysis}</div>
</div>

</div>

<style>
.enhanced-trends .impact-badge {{
    font-size: 0.85em;
}}
.enhanced-trends .high-impact {{ background: #fef2f2; border-color: #ef4444; color: #dc2626; }}
.enhanced-trends .medium-high-impact {{ background: #fff7ed; border-color: #f97316; color: #ea580c; }}
.enhanced-trends .medium-impact {{ background: #fefce8; border-color: #eab308; color: #ca8a04; }}
.enhanced-trends .standard-impact {{ background: #f0fdf4; border-color: #22c55e; color: #16a34a; }}
.enhanced-trends .low-impact {{ background: #f8fafc; border-color: #64748b; color: #475569; }}
.enhanced-trends .unknown-impact {{ background: #f1f5f9; border-color: #94a3b8; color: #64748b; }}

.enhanced-trends .impact-legend {{
    padding: 0.25rem 0.5rem;
    border-radius: 0.25rem;
    border: 1px solid;
    font-size: 0.75em;
    white-space: nowrap;
}}

.enhanced-trends .enhanced-trend-row:hover {{
    background-color: #f9fafb;
}}

.enhanced-trends .journal-info {{
    font-size: 0.9em;
}}
</style>
"""
        
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"ì—°êµ¬ ë™í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë¶„ì„ ì˜¤ë¥˜: {error_msg}")
        return f"""
<div class="research-trends">
<h3>ğŸ“Š ì—°êµ¬ ë™í–¥ ë¶„ì„ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ğŸ’¡ <strong>ëŒ€ì•ˆ:</strong> "APIë¡œ {topic} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.
</p>
</div>
"""

# í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ê²€ìƒ‰ í•¨ìˆ˜
@tool  
def test_paper_search(query: str = "aspirin") -> str:
    """í…ŒìŠ¤íŠ¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ - ê°„ë‹¨í•œ ê²€ìƒ‰ì–´ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        print(f"[TEST] í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        result = search_research_papers_sync(query, 3)  # 3ê°œë§Œ ê²€ìƒ‰
        print(f"[TEST] í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ")
        return result
    except Exception as e:
        return f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

@tool
def api_paper_search_tool(query: str) -> str:
    """NCBI E-utilities APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ê³¼ ì €ë„ IF ì ìˆ˜(SJR ê¸°ë°˜) ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 403 ì˜¤ë¥˜ ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ê° ë…¼ë¬¸ì˜ ì €ë„ ì˜í–¥ë ¥ì„ ìë™ ë¶„ì„í•©ë‹ˆë‹¤. HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”."""
    try:
        print(f"[API] APIë¥¼ í†µí•œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        import requests
        import xml.etree.ElementTree as ET
        from urllib.parse import quote
        
        # Step 1: ê²€ìƒ‰í•˜ì—¬ ë…¼ë¬¸ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': 5,  # ê¸°ë³¸ê°’ìœ¼ë¡œ 5ê°œ ì„¤ì •
            'retmode': 'xml',
            'datetype': 'pdat',
            'reldate': 1825,  # ìµœê·¼ 5ë…„ (365 * 5)
            'sort': 'relevance'
        }
        
        print(f"[API] ê²€ìƒ‰ ìš”ì²­ ì¤‘...")
        search_response = requests.get(search_url, params=search_params, timeout=30)
        
        if search_response.status_code != 200:
            return f"API ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {search_response.status_code}"
        
        # XML íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ID ì¶”ì¶œ
        search_root = ET.fromstring(search_response.content)
        id_list = search_root.find('.//IdList')
        
        if id_list is None or len(id_list) == 0:
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š API ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> ë” ì¼ë°˜ì ì¸ ì˜ì–´ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš” (ì˜ˆ: "cancer", "treatment", "therapy")
</p>
</div>
"""
        
        paper_ids = [id_elem.text for id_elem in id_list.findall('Id')]
        print(f"[API] ë°œê²¬ëœ ë…¼ë¬¸ ID: {len(paper_ids)}ê°œ")
        
        # Step 2: ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        fetch_params = {
            'db': 'pubmed',
            'id': ','.join(paper_ids),
            'retmode': 'xml'
        }
        
        print(f"[API] ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìš”ì²­ ì¤‘...")
        fetch_response = requests.get(fetch_url, params=fetch_params, timeout=30)
        
        if fetch_response.status_code != 200:
            return f"API ìƒì„¸ ì •ë³´ ìš”ì²­ ì‹¤íŒ¨: HTTP {fetch_response.status_code}"
        
        # XML íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        fetch_root = ET.fromstring(fetch_response.content)
        articles = fetch_root.findall('.//PubmedArticle')
        
        if not articles:
            return f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[API] ë…¼ë¬¸ ìƒì„¸ ì •ë³´ íŒŒì‹± ì¤‘: {len(articles)}ê°œ")
        
        # ë…¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ LLMìœ¼ë¡œ ìš”ì•½
        papers_data = []
        for i, article in enumerate(articles[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ ì²˜ë¦¬
            try:
                # ì œëª© ì¶”ì¶œ
                title_elem = article.find('.//ArticleTitle')
                title = title_elem.text if title_elem is not None and title_elem.text else "ì œëª© ì—†ìŒ"
                
                # ì´ˆë¡ ì¶”ì¶œ
                abstract_elem = article.find('.//Abstract/AbstractText')
                abstract = abstract_elem.text if abstract_elem is not None and abstract_elem.text else "ì´ˆë¡ ì—†ìŒ"
                
                # ì €ë„ëª… ì¶”ì¶œ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!)
                journal_title_elem = article.find('.//Journal/Title')
                journal_iso_elem = article.find('.//Journal/ISOAbbreviation')
                
                if journal_title_elem is not None and journal_title_elem.text:
                    journal_name = journal_title_elem.text.strip()
                elif journal_iso_elem is not None and journal_iso_elem.text:
                    journal_name = journal_iso_elem.text.strip()
                else:
                    journal_name = "Unknown"
                
                # ë°œí–‰ì—°ë„ ì¶”ì¶œ
                year_elem = article.find('.//PubDate/Year')
                year = year_elem.text if year_elem is not None and year_elem.text else "ì—°ë„ ë¯¸ìƒ"
                
                # PMID ì¶”ì¶œ
                pmid_elem = article.find('.//PMID')
                pmid = pmid_elem.text if pmid_elem is not None and pmid_elem.text else ""
                pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "#"
                
                papers_data.append({
                    'title': title,
                    'abstract': abstract,
                    'journal': journal_name,  # ì €ë„ëª… ì¶”ê°€!
                    'year': year,
                    'url': pubmed_url
                })
                
            except Exception as e:
                print(f"[API] ë…¼ë¬¸ {i} íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                continue
        
        if not papers_data:
            return "ë…¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[API] LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½ ì‹œì‘...")
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì„ ìš”ì•½
        papers_info = ""
        for i, paper in enumerate(papers_data, 1):
            papers_info += f"""
ë…¼ë¬¸ {i}:
ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}
ì—°ë„: {paper['year']}
URL: {paper['url']}

---
"""
        
        summary_prompt = f"""
ë‹¤ìŒ {len(papers_data)}ê°œì˜ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ê° ë…¼ë¬¸ë§ˆë‹¤ 2-3ì¤„ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.

{papers_info}

ê° ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ë…¼ë¬¸1||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
ë…¼ë¬¸2||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
...

ì£¼ìš”ë‚´ìš©ìš”ì•½ì€ ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ 2-3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ìš”ì•½ ìˆ˜í–‰
        summary_result = llm.invoke(summary_prompt)
        summary_lines = summary_result.content.strip().split('\n')
        
        # ì €ë„ë³„ SJR ì ìˆ˜ ë¶„ì„ ì¶”ê°€ (API ê²€ìƒ‰ìš©)
        print(f"[API] ì €ë„ SJR ì ìˆ˜ ë¶„ì„ ì¤‘...")
        enhanced_papers_data = []
        
        for paper in papers_data:
            # PubMedì—ì„œ ì¶”ì¶œí•œ ì‹¤ì œ ì €ë„ëª… ì‚¬ìš©
            actual_journal = paper.get('journal', 'Unknown')
            estimated_journal = actual_journal
            best_match_metrics = None
            
            print(f"[API] ì‹¤ì œ ì €ë„ëª…: {actual_journal}")
            
            # ì‹¤ì œ ì €ë„ëª…ìœ¼ë¡œ SJR ë©”íŠ¸ë¦­ ì¡°íšŒ
            if actual_journal and actual_journal != "Unknown":
                # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
                best_match_metrics = scimago_analyzer.get_journal_metrics_fast(actual_journal)
                
                # 2ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ì´ ì•ˆë˜ë©´ ì•½ê°„ì˜ ë³€í˜•ì„ ì‹œë„
                if not best_match_metrics or best_match_metrics.get('sjr_score', 0) == 0:
                    # ì €ë„ëª… ì •ë¦¬ (ì•½ì–´, íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)
                    cleaned_journal = actual_journal.lower().replace('&', 'and').replace('.', '').strip()
                    if cleaned_journal != actual_journal.lower():
                        estimated_journal = cleaned_journal
                        best_match_metrics = scimago_analyzer.get_journal_metrics_fast(cleaned_journal)
                        if best_match_metrics and best_match_metrics.get('sjr_score', 0) > 0:
                            print(f"[API] ì •ë¦¬ëœ ì €ë„ëª…ìœ¼ë¡œ ë§¤ì¹­ ì„±ê³µ: {actual_journal} â†’ {cleaned_journal}")
            
            # ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
            impact_score = 0.0
            impact_tier = "ğŸ“ Unknown"
            impact_class = "unknown-impact"
            
            if best_match_metrics:
                impact_score = scimago_analyzer.calculate_impact_score(best_match_metrics, 0)
                impact_tier, impact_class = scimago_analyzer.get_impact_tier(impact_score)
            
            enhanced_papers_data.append({
                'paper': paper,
                'journal': estimated_journal,
                'metrics': best_match_metrics,
                'impact_score': impact_score,
                'impact_tier': impact_tier,
                'impact_class': impact_class
            })
        
        # ì˜í–¥ë ¥ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        enhanced_papers_data.sort(key=lambda x: x['impact_score'], reverse=True)
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„± (SJR í¬í•¨, í¬ê¸° í™•ëŒ€)
        html_table = f"""
<div class="research-papers-table enhanced-api-search">
<h3>ğŸ”¬ API ê²€ìƒ‰ ê²°ê³¼ (SJR ë¶„ì„ í¬í•¨): {query}</h3>
<div class="search-summary" style="background-color: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
    <p style="color: #0369a1; margin: 0;">
    ğŸ“Š <strong>Enhanced API Search:</strong> NCBI E-utilities + Scimago Journal Rank í†µí•© ë¶„ì„<br>
    ğŸ¯ SJR ì ìˆ˜ ê¸°ë°˜ ì €ë„ í’ˆì§ˆ í‰ê°€ | ğŸ“ˆ Quartile ë¶„ì„ í¬í•¨
    </p>
</div>
<table class="papers-table enhanced-api-table" style="width: 100%; table-layout: fixed;">
<thead>
<tr>
<th style="width: 40%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 20%;">ì €ë„ (SJR)</th>
<th style="width: 35%;">ì£¼ìš” ë‚´ìš© (AI ìš”ì•½)</th>
<th style="width: 5%;">ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # í–¥ìƒëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í…Œì´ë¸” í–‰ìœ¼ë¡œ ì¶”ê°€ (API ê²€ìƒ‰ìš©)
        for i, enhanced_paper in enumerate(enhanced_papers_data):
            paper = enhanced_paper['paper']
            journal = enhanced_paper['journal']
            metrics = enhanced_paper['metrics']
            impact_score = enhanced_paper['impact_score']
            impact_tier = enhanced_paper['impact_tier']
            impact_class = enhanced_paper['impact_class']
            
            # LLM ìš”ì•½ì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì˜ ìš”ì•½ ì°¾ê¸°
            paper_summary = "ìš”ì•½ ìƒì„± ì¤‘..."
            for line in summary_lines:
                if f"ë…¼ë¬¸{i+1}||" in line:
                    parts = line.split("||")
                    if len(parts) >= 3:
                        paper_summary = parts[2].strip()
                    break
            
            # ìš”ì•½ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆë¡ ì‚¬ìš©
            if paper_summary == "ìš”ì•½ ìƒì„± ì¤‘..." or not paper_summary:
                paper_summary = paper['abstract'][:180] + "..." if len(paper['abstract']) > 180 else paper['abstract']
            
            # ì €ë„ ì •ë³´ êµ¬ì„±
            if metrics and metrics.get('sjr_score', 0) > 0:
                journal_info = f"""{journal}<br>
                <small style='color: #10b981; font-weight: 600; font-size: 0.85em;'>
                SJR: {metrics['sjr_score']:.2f} ({metrics['quartile']})
                </small>"""
            else:
                journal_info = f"""{journal}<br>
                <small style='color: #999; font-size: 0.8em;'>SJR ì •ë³´ ì—†ìŒ</small>"""
            
            html_table += f"""
<tr class="enhanced-api-row">
<td class="paper-title" style="word-wrap: break-word; padding: 0.8rem;"><strong>{paper['title']}</strong><br><small style="color: #666;">({paper['year']})</small></td>
<td class="journal-info" style="word-wrap: break-word; padding: 0.8rem;">{journal_info}</td>
<td class="paper-summary" style="word-wrap: break-word; padding: 0.8rem; line-height: 1.4;">{paper_summary}</td>
<td style="padding: 0.5rem;"><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸</a></td>
</tr>
"""
        
        html_table += f"""
</tbody>
</table>

</div>

<style>
.enhanced-api-search .impact-badge {{
    font-size: 0.75em;
}}
.enhanced-api-search .high-impact {{ background: #fef2f2; border-color: #ef4444; color: #dc2626; }}
.enhanced-api-search .medium-high-impact {{ background: #fff7ed; border-color: #f97316; color: #ea580c; }}
.enhanced-api-search .medium-impact {{ background: #fefce8; border-color: #eab308; color: #ca8a04; }}
.enhanced-api-search .standard-impact {{ background: #f0fdf4; border-color: #22c55e; color: #16a34a; }}
.enhanced-api-search .low-impact {{ background: #f8fafc; border-color: #64748b; color: #475569; }}
.enhanced-api-search .unknown-impact {{ background: #f1f5f9; border-color: #94a3b8; color: #64748b; }}

.enhanced-api-search .impact-legend {{
    padding: 0.25rem 0.5rem;
    border-radius: 0.25rem;
    border: 1px solid;
    font-size: 0.75em;
    white-space: nowrap;
}}

.enhanced-api-search .enhanced-api-row:hover {{
    background-color: #f9fafb;
}}

.enhanced-api-search .enhanced-api-table {{
    min-width: 1200px;
}}

.enhanced-api-search .paper-title {{
    font-size: 0.95em;
    line-height: 1.3;
}}

.enhanced-api-search .paper-summary {{
    font-size: 0.9em;
    line-height: 1.4;
}}

.enhanced-api-search .journal-info {{
    font-size: 0.85em;
    line-height: 1.2;
}}
</style>
"""
        
        print(f"[API] HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"API ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[API] ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š API ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}
</p>
</div>
"""

@tool
def get_pubmed_search_tips() -> str:
    """PubMed ê²€ìƒ‰ì´ ì˜ ì•ˆë  ë•Œ ë„ì›€ì´ ë˜ëŠ” íŒì„ ì œê³µí•©ë‹ˆë‹¤."""
    return """
<div class="search-tips">
<h3>ğŸ” PubMed ê²€ìƒ‰ ìµœì í™” íŒ</h3>

<div style="background-color: #f0fdf4; padding: 1rem; border: 1px solid #bbf7d0; border-radius: 0.5rem; margin: 1rem 0;">
<h4 style="color: #166534; margin-bottom: 0.5rem;">âœ… íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´</h4>
<ul style="color: #166534; margin-left: 1rem;">
<li><strong>ì¼ë°˜ì ì¸ ìš©ì–´ ì‚¬ìš©:</strong> "cancer", "treatment", "therapy"</li>
<li><strong>ì˜ì–´ ê²€ìƒ‰ì–´:</strong> í•œê¸€ë³´ë‹¤ ì˜ì–´ê°€ ë” ë§ì€ ê²°ê³¼</li>
<li><strong>MeSH ìš©ì–´:</strong> ì˜í•™ í‘œì¤€ ìš©ì–´ í™œìš©</li>
<li><strong>ë™ì˜ì–´ í¬í•¨:</strong> "drug OR medication OR pharmaceutical"</li>
</ul>
</div>

<div style="background-color: #fef2f2; padding: 1rem; border: 1px solid #fecaca; border-radius: 0.5rem; margin: 1rem 0;">
<h4 style="color: #dc2626; margin-bottom: 0.5rem;">âŒ í”¼í•´ì•¼ í•  ê²€ìƒ‰ì–´</h4>
<ul style="color: #dc2626; margin-left: 1rem;">
<li><strong>ë„ˆë¬´ êµ¬ì²´ì :</strong> "doxorubicin cardiotoxicity prevention mechanism"</li>
<li><strong>ë¸Œëœë“œëª…:</strong> ì¼ë°˜ëª… ì‚¬ìš© ê¶Œì¥</li>
<li><strong>ë³µì¡í•œ ì¡°í•©:</strong> 3ê°œ ì´ìƒì˜ AND ì¡°ê±´</li>
</ul>
</div>

<div style="background-color: #f8f9fa; padding: 1rem; border: 1px solid #e9ecef; border-radius: 0.5rem;">
<h4 style="color: #495057; margin-bottom: 0.5rem;">ğŸ¯ ì¶”ì²œ ê²€ìƒ‰ì–´ ì˜ˆì‹œ</h4>
<ul style="color: #495057; margin-left: 1rem;">
<li>"cancer treatment"</li>
<li>"immunotherapy"</li>
<li>"drug delivery"</li>
<li>"chemotherapy"</li>
<li>"biomarker"</li>
<li>"clinical trial"</li>
</ul>
</div>
</div>
"""

@tool
def enhanced_paper_search_with_sjr(query: str, max_papers: int = 5) -> str:
    """SJR ì˜í–¥ë ¥ ë¶„ì„ì´ í¬í•¨ëœ í–¥ìƒëœ ë…¼ë¬¸ ê²€ìƒ‰. ê¸°ì¡´ ê²€ìƒ‰ë³´ë‹¤ ë” ìì„¸í•œ ì €ë„ ì •ë³´ì™€ ì˜í–¥ë ¥ ì ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    try:
        print(f"[ENHANCED] SJR ë¶„ì„ í¬í•¨ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ê¸°ë³¸ ë…¼ë¬¸ ê²€ìƒ‰ ìˆ˜í–‰
        results = search_research_papers_sync(query, max_papers)
        
        if "ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in results:
            return results
        
        # HTMLì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹±ì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í–¥ìƒì‹œí‚´
        enhanced_html = results.replace(
            '<h3>ğŸ“š ì—°êµ¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼:', 
            '<h3>ğŸ”¬ SJR ë¶„ì„ í¬í•¨ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼:'
        )
        
        # SJR ë¶„ì„ ì •ë³´ ì¶”ê°€
        sjr_info = """
        """
        
        enhanced_html = enhanced_html.replace('</div>', sjr_info + '</div>')
        
        return enhanced_html
        
    except Exception as e:
        error_msg = f"SJR ë¶„ì„ í¬í•¨ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        print(f"[ENHANCED] {error_msg}")
        return f"""
        <div class="research-papers-table">
        <h3>ğŸ”¬ SJR ë¶„ì„ í¬í•¨ ê²€ìƒ‰ ì˜¤ë¥˜</h3>
        <p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
        {error_msg}
        </p>
        </div>
        """

tools = [
    generate_molecule_image, 
    name_to_smiles_cir, 
    paper_search_tool,  # ë‹¨ìˆœí™”ëœ ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬
    api_paper_search_tool,  # ë‹¨ìˆœí™”ëœ API ê²€ìƒ‰ ë„êµ¬
    enhanced_paper_search_with_sjr,  # ìƒˆë¡œìš´ SJR ë¶„ì„ í¬í•¨ ê²€ìƒ‰
    analyze_chemical_paper,
    analyze_chemical_research_trends,  # ì´ì œ SJR ë¶„ì„ í¬í•¨
    get_pubmed_search_tips
]

# LangGraph ì„¤ì •
class State(TypedDict):
    messages: Annotated[list, add_messages]

class BasicToolNode:
    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result) if not isinstance(tool_result, str) else tool_result,
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}

# ê·¸ë˜í”„ êµ¬ì„±
graph_builder = StateGraph(State)
tool_node = BasicToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)

llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

def route_tools(state: State):
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END

graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    {"tools": "tools", END: END},
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatMessage(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    image: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def get_chat_interface():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except Exception as e:
        return HTMLResponse(f"<h1>Error loading page: {str(e)}</h1>")

@app.get("/favicon.ico")
async def get_favicon():
    """Favicon ìš”ì²­ ì²˜ë¦¬ - 404 ì—ëŸ¬ ë°©ì§€"""
    # ê°„ë‹¨í•œ íˆ¬ëª… 1x1 í”½ì…€ PNG ì´ë¯¸ì§€ (Base64)
    transparent_png = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAGA60e6kgAAAABJRU5ErkJggg=="
    
    import base64
    from fastapi.responses import Response
    
    png_data = base64.b64decode(transparent_png)
    return Response(content=png_data, media_type="image/png")

@app.post("/chat")
async def chat_endpoint(chat_message: ChatMessage):
    try:
        # HJ ChemAgent ì‹¤í–‰
        result = graph.invoke({"messages": [{"role": "user", "content": chat_message.message}]})
        
        # HTML ë„êµ¬ ê²°ê³¼ ì§ì ‘ ê°ì§€ ë° ìš°íšŒ ì²˜ë¦¬
        html_tool_result = None
        for message in result["messages"]:
            if hasattr(message, 'name') and message.name in ['api_paper_search_tool', 'paper_search_tool', 'analyze_chemical_research_trends']:
                tool_content = message.content
                if isinstance(tool_content, str) and (
                    '<div class="research-papers-table">' in tool_content or 
                    '<div class="research-trends">' in tool_content or
                    '<div class="paper-analysis">' in tool_content or
                    '<table' in tool_content
                ):
                    html_tool_result = tool_content
                    print(f"[DEBUG] HTML ë„êµ¬ ê²°ê³¼ ì§ì ‘ ê°ì§€ë¨ - ë„êµ¬: {message.name}, ê¸¸ì´: {len(tool_content)}")
                    break
        
        # HTML ë„êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ LLM ì‘ë‹µ ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©
        if html_tool_result:
            response_text = html_tool_result
            print(f"[DEBUG] LLM ìš°íšŒí•˜ì—¬ ë„êµ¬ ê²°ê³¼ ì§ì ‘ ì‚¬ìš©")
        else:
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            last_message = result["messages"][-1]
            response_text = last_message.content
        
        # HTML í…Œì´ë¸” ì§ì ‘ ê°ì§€ ë° ì²˜ë¦¬ (ë°±ì—”ë“œì—ì„œ ì¶”ê°€ ë³´ì¥)
        print(f"[DEBUG] ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
        print(f"[DEBUG] ì‘ë‹µ ì‹œì‘ 100ì: {response_text[:100]}")
        
        # HTML íƒœê·¸ ê°ì§€
        has_html = ("<div" in response_text and "</div>" in response_text) or ("<table" in response_text and "</table>" in response_text)
        
        if has_html:
            print(f"[DEBUG] HTML íƒœê·¸ ê°ì§€ë¨")
            
            # HTML ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
            html_start = response_text.find("<div")
            if html_start == -1:
                html_start = response_text.find("<table")
            
            if html_start > 0:
                # HTML ì•ì— ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì œê±°
                original_length = len(response_text)
                response_text = response_text[html_start:].strip()
                print(f"[DEBUG] HTML ì•ì˜ í…ìŠ¤íŠ¸ ì œê±°ë¨ - ì›ë˜ ê¸¸ì´: {original_length}, ìƒˆ ê¸¸ì´: {len(response_text)}")
            
            # ì¶”ê°€ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ HTMLì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if any(phrase in response_text for phrase in ["ê²°ê³¼ì…ë‹ˆë‹¤", "ë¶„ì„í–ˆìŠµë‹ˆë‹¤", "ë³´ê³ ì„œì…ë‹ˆë‹¤", "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤"]):
                print(f"[DEBUG] ì¶”ê°€ ì„¤ëª… í…ìŠ¤íŠ¸ ê°ì§€ë¨, HTML ì¶”ì¶œ ì‹œë„")
                import re
                
                # ë” í¬ê´„ì ì¸ HTML ë§¤ì¹­
                html_patterns = [
                    r'<div[^>]*class="research-trends"[^>]*>.*?</div>',
                    r'<div[^>]*class="research-papers-table"[^>]*>.*?</div>',
                    r'<div[^>]*class="paper-analysis"[^>]*>.*?</div>',
                    r'<table[^>]*>.*?</table>'
                ]
                
                for pattern in html_patterns:
                    html_match = re.search(pattern, response_text, re.DOTALL)
                    if html_match:
                        extracted_html = html_match.group(0)
                        print(f"[DEBUG] íŒ¨í„´ '{pattern[:30]}...'ë¡œ HTML ì¶”ì¶œë¨ - ê¸¸ì´: {len(extracted_html)}")
                        response_text = extracted_html
                        break
                else:
                    print(f"[DEBUG] HTML íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨, ì›ë³¸ ìœ ì§€")
        else:
            print(f"[DEBUG] HTML íƒœê·¸ ì—†ìŒ, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
        
        print(f"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
        print(f"[DEBUG] ìµœì¢… ì‘ë‹µ ì‹œì‘ 200ì: {response_text[:200]}")
        
        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì‘ë‹µì¸ì§€ í™•ì¸
        image_data = None
        for message in result["messages"]:
            if hasattr(message, 'content') and isinstance(message.content, str):
                # JSONìœ¼ë¡œ ì¸ì½”ë”©ëœ íˆ´ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                try:
                    import json
                    content = json.loads(message.content)
                    if isinstance(content, str) and content.startswith("data:image/png;base64,"):
                        image_data = content
                        break
                except:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ í™•ì¸
                    if message.content.startswith("data:image/png;base64,"):
                        image_data = message.content
                        break
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ base64 ì´ë¯¸ì§€ ë°ì´í„° ì œê±° (í…ìŠ¤íŠ¸ë¡œ í‘œì‹œë˜ì§€ ì•Šë„ë¡)
        if image_data and image_data in response_text:
            response_text = response_text.replace(image_data, "").strip()
        
        # base64 ë°ì´í„°ê°€ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì œê±°í•˜ê³  ê°„ë‹¨í•œ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
        if "data:image/png;base64," in response_text:
            response_text = "ë¶„ìì˜ êµ¬ì¡° ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        
        return ChatResponse(response=response_text, image=image_data)
        
    except Exception as e:
        return ChatResponse(
            response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ì •ì  íŒŒì¼ ì„œë¹™ (í•„ìš”ì‹œ)
# try:
#     app.mount("/static", StaticFiles(directory="static"), name="static")
# except Exception as e:
#     print(f"Static files mounting error: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8002)
