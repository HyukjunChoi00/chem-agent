from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import os
import json
import base64
from io import BytesIO

# HJ ChemAgent ê´€ë ¨ ì„í¬íŠ¸
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import tool
from typing import Annotated
from langchain_core.messages import BaseMessage, ToolMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
import requests
import urllib.parse

# ë…¼ë¬¸ ê²€ìƒ‰ ê´€ë ¨ ì„í¬íŠ¸
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup, Comment
import asyncio
import re
from urllib.parse import urlparse, urljoin
import nest_asyncio
nest_asyncio.apply()
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

app = FastAPI(title="HJ ChemAgent Simple")

# LLM ì„¤ì •
os.environ["GOOGLE_API_KEY"] = 'AIzaSyAPf9KX4Ea_MKiC_tYfm8OUlv_A71BUz1I'
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-001",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    system_instruction="""ë‹¹ì‹ ì€ í™”í•™ ì—°êµ¬ ë³´ì¡° AIì…ë‹ˆë‹¤. 
    
    ğŸš¨ CRITICAL RULE - HTML ì™„ì „ ì¶œë ¥:
    ë„êµ¬ê°€ HTMLì„ ë°˜í™˜í•˜ë©´ (íŠ¹íˆ <div>, <table> íƒœê·¸):
    1. HTMLì„ ì™„ì „íˆ, ëê¹Œì§€, ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”
    2. HTMLì„ ìë¥´ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”
    3. ì ˆëŒ€ ì¶”ê°€ ì„¤ëª… ê¸ˆì§€: "ê²°ê³¼ì…ë‹ˆë‹¤", "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤", "ë¶„ì„í–ˆìŠµë‹ˆë‹¤" ë“±
    4. HTML ì•ë’¤ì— ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ë¶™ì´ì§€ ë§ˆì„¸ìš”
    5. ë„êµ¬ê°€ ì™„ì „í•œ HTML í…Œì´ë¸”ì„ ë°˜í™˜í–ˆë‹¤ë©´ ê·¸ ì „ì²´ë¥¼ ì¶œë ¥í•˜ì„¸ìš”
    
    ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
    ë„êµ¬ ì¶œë ¥: "<div class="research-papers-table"><h3>ğŸ“š API ê²€ìƒ‰ ê²°ê³¼...</h3><table>...</table></div>"
    â†’ ë‹¹ì‹ ì˜ ì‘ë‹µ: <div class="research-papers-table"><h3>ğŸ“š API ê²€ìƒ‰ ê²°ê³¼...</h3><table>...</table></div>
    
    ì ˆëŒ€ ê¸ˆì§€:
    â†’ "doxorubicinì— ëŒ€í•œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. | ë…¼ë¬¸ ì œëª© | ì£¼ìš” ë‚´ìš©"
    â†’ HTMLì„ ì¤‘ê°„ì— ìë¥´ëŠ” í–‰ìœ„
    
    HTMLì´ ì•„ë‹Œ ì¼ë°˜ ì§ˆë¬¸ì€ í‰ì†Œì²˜ëŸ¼ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.""",
)

# ë…¼ë¬¸ ê²€ìƒ‰ í´ë˜ìŠ¤ ì •ì˜ (ì˜ ì‘ë™í•˜ëŠ” ì›ë³¸ ë²„ì „)
class paper_search:
    def __init__(self, max_articles=5):
        self.max_articles = max_articles
        self.base_url = "https://pubmed.ncbi.nlm.nih.gov/"
        self.playwright = None
        self.browser = None
        self.context = None

    async def setup_browser(self):
        if self.playwright is None:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--disable-gpu'
                ]
            )
            self.context = await self.browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )

    async def close_browser(self):
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        self.playwright = None
        self.browser = None
        self.context = None

    def clean_title(self, title: str) -> str:
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^\W+|\W+$', '', title)
        return title if len(title) > 10 else ""

    async def extract_articles(self, page, query: str) -> list:
        articles = []
        print(f"ë…¼ë¬¸ ì¶”ì¶œ ì‹œì‘... ìµœëŒ€ {self.max_articles}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤.")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        try:
            await page.wait_for_selector('#search-results', timeout=10000)
            print("ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ ë¡œë”© ì™„ë£Œ")
        except:
            print("ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜¬ë°”ë¥¸ PubMed selectorë“¤
        selectors_to_try = [
            'a.docsum-title',  # ì‹¤ì œ êµ¬ì¡°: <a class="docsum-title" href="...">
            '.docsum-title',   # í´ë˜ìŠ¤ë§Œìœ¼ë¡œë„ ê°€ëŠ¥
            '#search-results a.docsum-title',  # ë” êµ¬ì²´ì ì¸ ê²½ë¡œ
            'a[data-ga-category="result_click"]',  # ë°ì´í„° ì†ì„± í™œìš©
            'a[data-article-id]',  # ë…¼ë¬¸ ID ì†ì„±ì´ ìˆëŠ” ë§í¬
            '[data-ga-action]'  # GA ì•¡ì…˜ ì†ì„±ì´ ìˆëŠ” ìš”ì†Œ
        ]
        
        # ëª¨ë“  ë…¼ë¬¸ ë§í¬ë¥¼ í•œ ë²ˆì— ì°¾ê¸°
        all_links = []
        for selector in selectors_to_try:
            try:
                links = await page.query_selector_all(selector)
                if links:
                    print(f"'{selector}'ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                    all_links = links
                    break
            except Exception as e:
                print(f"Selector '{selector}' ì˜¤ë¥˜: {str(e)}")
                continue
        
        if not all_links:
            print("ì–´ë–¤ selectorë¡œë„ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # HTML êµ¬ì¡° ë””ë²„ê¹…
            try:
                html_content = await page.inner_html('#search-results')
                print("ê²€ìƒ‰ ê²°ê³¼ HTML ì¼ë¶€:")
                print(html_content[:500] + "...")
            except:
                print("HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì°¾ì€ ë§í¬ë“¤ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        for i in range(min(len(all_links), self.max_articles)):
            try:
                link = all_links[i]
                print(f"ë…¼ë¬¸ {i+1} ì¶”ì¶œ ì¤‘...")
                
                title = await link.inner_text()
                url = await link.get_attribute('href')
                
                print(f"ë…¼ë¬¸ {i+1} ì œëª©: {title[:50]}...")
                print(f"ë…¼ë¬¸ {i+1} URL: {url}")
                
                # URL ì •ê·œí™”
                if url and not url.startswith("http"):
                    url = urljoin(self.base_url, url)
                
                title = self.clean_title(title)
                if title and url:
                    articles.append({"title": title, "url": url})
                    print(f"ë…¼ë¬¸ {i+1}: ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œë¨")
                else:
                    print(f"ë…¼ë¬¸ {i+1}: ì œëª© ë˜ëŠ” URLì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                    
            except Exception as e:
                print(f"ë…¼ë¬¸ {i+1} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
                
        print(f"ì´ {len(articles)}ê°œì˜ ë…¼ë¬¸ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return articles

    async def extract_abstract(self, url: str) -> str:
        """ë…¼ë¬¸ í˜ì´ì§€ì—ì„œ ì´ˆë¡ì„ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        page = await self.context.new_page()
        try:
            print(f"ì´ˆë¡ ì¶”ì¶œì„ ìœ„í•´ í˜ì´ì§€ ì ‘ì†: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=20000)
            await asyncio.sleep(3)
            
            # ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì´ˆë¡ ì°¾ê¸°
            abstract_text = ""
            
            # ë°©ë²• 1: #abstract .abstract-content ì‚¬ìš©
            abstract_content = await page.query_selector('#abstract .abstract-content')
            if abstract_content:
                abstract_text = await abstract_content.inner_text()
                print("ë°©ë²• 1ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
            else:
                # ë°©ë²• 2: #eng-abstract ì‚¬ìš© (ì˜ì–´ ì´ˆë¡)
                eng_abstract = await page.query_selector('#eng-abstract')
                if eng_abstract:
                    abstract_text = await eng_abstract.inner_text()
                    print("ë°©ë²• 2ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
                else:
                    # ë°©ë²• 3: #abstract ì „ì²´ ì‚¬ìš©
                    abstract_element = await page.query_selector('#abstract')
                    if abstract_element:
                        abstract_text = await abstract_element.inner_text()
                        # "Abstract" ì œëª© ì œê±°
                        abstract_text = re.sub(r'^Abstract\s*', '', abstract_text.strip())
                        print("ë°©ë²• 3ìœ¼ë¡œ ì´ˆë¡ ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print("ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ì´ˆë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return "ì´ˆë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return abstract_text.strip() if abstract_text else "ì´ˆë¡ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"ì´ˆë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ì´ˆë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        finally:
            await page.close()

    async def run_pipeline(self, query: str) -> list:
        await self.setup_browser()
        search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query}&filter=datesearch.y_5&filter=simsearch1.fha&filter=pubt.clinicaltrial"
        
        print(f"ê²€ìƒ‰ URL: {search_url}")
        
        # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        articles = []
        
        for attempt in range(max_retries):
            page = await self.context.new_page()
            try:
                print(f"ì‹œë„ {attempt + 1}/{max_retries}")
                
                # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                await asyncio.sleep(2 * (attempt + 1))  # ì ì§„ì  ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
                
                print("ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                
                # í˜ì´ì§€ ì œëª© í™•ì¸
                page_title = await page.title()
                print(f"í˜ì´ì§€ ì œëª©: {page_title}")
                
                # 403 ì˜¤ë¥˜ ê°ì§€
                if "403" in page_title or "Forbidden" in page_title:
                    print(f"403 ì˜¤ë¥˜ ê°ì§€ë¨. ì‹œë„ {attempt + 1}")
                    if attempt < max_retries - 1:
                        await page.close()
                        continue
                    else:
                        print("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                        await page.close()
                        break
                
                # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                articles = await self.extract_articles(page, query)
                await page.close()
                break
                
            except Exception as e:
                print(f"ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                await page.close()
                if attempt < max_retries - 1:
                    continue
                else:
                    print("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                    break

        results = []
        print(f"\nì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
        
        for i, article in enumerate(articles, 1):
            print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
            # ì´ˆë¡ ì¶”ì¶œì„ ìœ„í•´ ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
            abstract = await self.extract_abstract(article["url"])
            results.append({
                "title": article["title"],
                "url": article["url"],
                "abstract": abstract
            })

        await self.close_browser()
        return results

# ì „ì—­ ë…¼ë¬¸ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤
paper_searcher = paper_search(max_articles=5)

# í™”í•™ ë„êµ¬ë“¤ ì •ì˜
@tool
def name_to_smiles_cir(name: str) -> str:
    """IUPAC Nameì„ SMILESë¡œ ë³€í™˜"""
    encoded_name = urllib.parse.quote(name)
    url = f"https://cactus.nci.nih.gov/chemical/structure/{encoded_name}/smiles"
    
    try:
        response = requests.get(url, timeout=10)
        smiles = response.text.strip()
        return smiles
    except Exception as e:
        return f"Error converting name to SMILES: {str(e)}"

@tool
def generate_molecule_image(smiles: str) -> str:
    """Generate and return base64 encoded image of a molecule from its SMILES string."""
    try:
        # RDKit ì„í¬íŠ¸ ì‹œë„
        try:
            from rdkit import Chem
            from rdkit.Chem import Draw
        except ImportError:
            return "RDKitì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ RDKitì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:\n\nconda install -c conda-forge rdkit\n\në˜ëŠ”\n\npip install rdkit"
        
        # SMILESë¥¼ ë¶„ìë¡œ ë³€í™˜
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return "Invalid SMILES string."
        
        # ë¶„ì ì´ë¯¸ì§€ ìƒì„±
        image = Draw.MolToImage(mol, size=(300, 300))
        
        # PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        return f"data:image/png;base64,{img_base64}"
        
    except Exception as e:
        return f"Error generating molecule image: {str(e)}"

def search_research_papers_trends_sync(query: str, max_papers: int = 10) -> str:
    """ì—°êµ¬ ë™í–¥ ë¶„ì„ì„ ìœ„í•œ ë” ë„“ì€ ë²”ìœ„ì˜ ë…¼ë¬¸ ê²€ìƒ‰ (í•„í„° ì™„í™”)"""
    try:
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        import asyncio
        
        async def _search_papers_for_trends():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨ (ë™í–¥ ë¶„ì„ìš©)")
            
            # í•„í„°ë¥¼ ì™„í™”í•œ ê²€ìƒ‰ URL (ëª¨ë“  ë…¼ë¬¸ íƒ€ì… í¬í•¨, ìµœê·¼ 5ë…„)
            await searcher.setup_browser()
            search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query}&filter=datesearch.y_5"
            
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ê²€ìƒ‰ URL: {search_url}")
            
            # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            articles = []
            
            for attempt in range(max_retries):
                page = await searcher.context.new_page()
                try:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}/{max_retries}")
                    
                    # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                    await asyncio.sleep(2 * (attempt + 1))
                    
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                    await asyncio.sleep(5)
                    
                    print("[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                    
                    # í˜ì´ì§€ ì œëª© í™•ì¸
                    page_title = await page.title()
                    print(f"[DEBUG] í˜ì´ì§€ ì œëª©: {page_title}")
                    
                    # 403 ì˜¤ë¥˜ ê°ì§€
                    if "403" in page_title or "Forbidden" in page_title:
                        print(f"[DEBUG] 403 ì˜¤ë¥˜ ê°ì§€ë¨. ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}")
                        if attempt < max_retries - 1:
                            await page.close()
                            continue
                        else:
                            print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                            await page.close()
                            break
                    
                    # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                    articles = await searcher.extract_articles(page, query)
                    await page.close()
                    break
                    
                except Exception as e:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    await page.close()
                    if attempt < max_retries - 1:
                        continue
                    else:
                        print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                        break

            results = []
            print(f"\n[DEBUG] ë™í–¥ ë¶„ì„ìš© ì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
            
            for i, article in enumerate(articles, 1):
                print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
                abstract = await searcher.extract_abstract(article["url"])
                results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "abstract": abstract
                })

            await searcher.close_browser()
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers_for_trends())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers_for_trends())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            results = asyncio.run(_search_papers_for_trends())
        
        if not results:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            return f"'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # ê¹”ë”í•œ HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = """
<div class="research-papers-table">
<h3>ğŸ“š ì—°êµ¬ ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 5%;">ë²ˆí˜¸</th>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì´ˆë¡ ìš”ì•½</th>
<th style="width: 10%;">ë§í¬</th>
</tr>
</thead>
<tbody>
""".format(query)
        
        for i, paper in enumerate(results, 1):
            # ì´ˆë¡ì„ 200ìë¡œ ì œí•œ
            abstract_summary = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
            
            html_table += f"""
<tr>
<td>{i}</td>
<td class="paper-title">{paper['title']}</td>
<td class="paper-abstract">{abstract_summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ë³´ê¸°</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
</div>
"""
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        return html_table
        
    except Exception as e:
        error_msg = f"ë™í–¥ ë¶„ì„ìš© ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def search_research_papers_sync(query: str, max_papers: int = 5) -> str:
    """í™”í•™ ì—°êµ¬ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë™ê¸° ë²„ì „)"""
    try:
        print(f"[DEBUG] ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        import asyncio
        
        async def _search_papers():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨")
            results = await searcher.run_pipeline(query)
            print(f"[DEBUG] ê²€ìƒ‰ ê²°ê³¼: {len(results) if results else 0}ê°œ ë…¼ë¬¸ ë°œê²¬")
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                import concurrent.futures
                import threading
                
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)  # íƒ€ì„ì•„ì›ƒì„ 120ì´ˆë¡œ ì¦ê°€
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            results = asyncio.run(_search_papers())
        
        if not results:
            print(f"[DEBUG] ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            # ë” ìƒì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š '{query}' ê²€ìƒ‰ ê²°ê³¼</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:<br><br>
â€¢ ë” ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš© (ì˜ˆ: "cancer treatment" ëŒ€ì‹  "cancer")<br>
â€¢ ì˜ì–´ ê²€ìƒ‰ì–´ ì‚¬ìš©<br>
â€¢ ë‹¤ë¥¸ í™”í•™ ë¬¼ì§ˆëª… ì‹œë„<br>
â€¢ ê²€ìƒ‰ì–´ë¥¼ ë” ê°„ë‹¨í•˜ê²Œ ë³€ê²½
</p>
</div>
"""
        
        print(f"[DEBUG] HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # ê¹”ë”í•œ HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = """
<div class="research-papers-table">
<h3>ğŸ“š ì—°êµ¬ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 5%;">ë²ˆí˜¸</th>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì´ˆë¡ ìš”ì•½</th>
<th style="width: 10%;">ë§í¬</th>
</tr>
</thead>
<tbody>
""".format(query)
        
        for i, paper in enumerate(results, 1):
            # ì´ˆë¡ì„ 200ìë¡œ ì œí•œ
            abstract_summary = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
            
            html_table += f"""
<tr>
<td>{i}</td>
<td class="paper-title">{paper['title']}</td>
<td class="paper-abstract">{abstract_summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ë³´ê¸°</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
</div>
"""
        
        print(f"[DEBUG] HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        return html_table
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
</p>
</div>
"""

@tool
def paper_search_tool(query: str) -> str:
    """ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë°˜í™˜ëœ HTMLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”."""
    try:
        print(f"[TOOL] ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì‹œì‘: {query}")
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        import asyncio
        
        async def _search_pipeline():
            pipeline = paper_search(max_articles=5)
            results = await pipeline.run_pipeline(query)
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_pipeline())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                results = loop.run_until_complete(_search_pipeline())
        except RuntimeError:
            results = asyncio.run(_search_pipeline())
        
        if not results:
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> "APIë¡œ {query} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¼ê³  ìš”ì²­í•˜ë©´ ë” ì•ˆì •ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</p>
</div>
"""
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì„ ìš”ì•½í•˜ê³  í‘œë¡œ ì •ë¦¬
        print(f"[TOOL] LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½ ì‹œì‘...")
        
        # ëª¨ë“  ë…¼ë¬¸ ì •ë³´ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ìš”ì•½ ìš”ì²­
        papers_info = ""
        for i, paper in enumerate(results, 1):
            papers_info += f"""
ë…¼ë¬¸ {i}:
ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}
URL: {paper['url']}

---
"""
        
        summary_prompt = f"""
ë‹¤ìŒ {len(results)}ê°œì˜ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ê° ë…¼ë¬¸ë§ˆë‹¤ 2-3ì¤„ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.

{papers_info}

ê° ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ë…¼ë¬¸1||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
ë…¼ë¬¸2||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
...

ì£¼ìš”ë‚´ìš©ìš”ì•½ì€ ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ 2-3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ìš”ì•½ ìˆ˜í–‰
        summary_result = llm.invoke(summary_prompt)
        summary_lines = summary_result.content.strip().split('\n')
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = f"""
<div class="research-papers-table">
<h3>ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 40%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì£¼ìš” ë‚´ìš© (AI ìš”ì•½)</th>
<th style="width: 10%;">ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # ìš”ì•½ëœ ë‚´ìš©ê³¼ ì›ë³¸ ë…¼ë¬¸ ì •ë³´ë¥¼ ë§¤ì¹­
        for i, paper in enumerate(results):
            # LLM ìš”ì•½ì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì˜ ìš”ì•½ ì°¾ê¸°
            paper_summary = "ìš”ì•½ ìƒì„± ì¤‘..."
            for line in summary_lines:
                if f"ë…¼ë¬¸{i+1}||" in line:
                    parts = line.split("||")
                    if len(parts) >= 3:
                        paper_summary = parts[2].strip()
                    break
            
            # ìš”ì•½ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆë¡ ì‚¬ìš©
            if paper_summary == "ìš”ì•½ ìƒì„± ì¤‘..." or not paper_summary:
                paper_summary = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
            
            html_table += f"""
<tr>
<td class="paper-title">{paper['title']}</td>
<td class="paper-summary">{paper_summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
<div style="margin-top: 1rem; padding: 0.5rem; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 0.5rem;">
<p style="color: #166534; font-size: 0.9rem;">
âœ… <strong>ê²€ìƒ‰ ì™„ë£Œ:</strong> ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. 403 ì˜¤ë¥˜ ì‹œ API ê²€ìƒ‰ì„ ì´ìš©í•˜ì„¸ìš”.
</p>
</div>
</div>
"""
        
        print(f"[TOOL] ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì™„ë£Œ: {len(results)}ê°œ ë…¼ë¬¸")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜: {str(e)}"
        print(f"[TOOL] ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ğŸ’¡ <strong>ëŒ€ì•ˆ:</strong> "APIë¡œ {query} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.
</p>
</div>
"""

# í™”í•™ ë…¼ë¬¸ ë¶„ì„ ë„êµ¬
@tool
def analyze_chemical_paper(paper_url: str) -> str:
    """PubMed ë…¼ë¬¸ URLì„ ë°›ì•„ì„œ í™”í•™ì  ê´€ì ì—ì„œ ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì‹œì‘: {paper_url}")
        
        # ë…¼ë¬¸ ë‚´ìš© ì¶”ì¶œ
        import asyncio
        
        async def _extract_paper_content():
            searcher = paper_search(max_articles=1)
            await searcher.setup_browser()
            
            try:
                # ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì´ˆë¡ê³¼ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                abstract = await searcher.extract_abstract(paper_url)
                
                # ì¶”ê°€ë¡œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                page = await searcher.context.new_page()
                await page.goto(paper_url, wait_until='domcontentloaded', timeout=20000)
                await asyncio.sleep(3)
                
                # ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
                title_element = await page.query_selector('h1.heading-title, .abstract-title, h1')
                title = await title_element.inner_text() if title_element else "ì œëª© ì—†ìŒ"
                
                # ì €ì ì •ë³´ ì¶”ì¶œ
                authors_elements = await page.query_selector_all('.authors .authors-list-item, .contrib-author')
                authors = []
                for author_elem in authors_elements[:5]:  # ìµœëŒ€ 5ëª…
                    try:
                        author_name = await author_elem.inner_text()
                        authors.append(author_name.strip())
                    except:
                        continue
                
                # ë°œí–‰ ì •ë³´ ì¶”ì¶œ
                pub_date_element = await page.query_selector('.cit, .publication-date, .pub-date')
                pub_date = await pub_date_element.inner_text() if pub_date_element else "ë‚ ì§œ ì—†ìŒ"
                
                await page.close()
                await searcher.close_browser()
                
                return {
                    "title": title,
                    "authors": authors,
                    "publication_date": pub_date,
                    "abstract": abstract
                }
                
            except Exception as e:
                await searcher.close_browser()
                raise e
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_extract_paper_content())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    paper_data = future.result(timeout=60)
            else:
                paper_data = loop.run_until_complete(_extract_paper_content())
        except RuntimeError:
            paper_data = asyncio.run(_extract_paper_content())
        
        # LLMì„ ì‚¬ìš©í•œ í™”í•™ì  ë¶„ì„
        analysis_prompt = f"""
ë‹¤ìŒ í™”í•™/ì˜í•™ ë…¼ë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {paper_data['title']}
ì €ì: {', '.join(paper_data['authors'][:3])}{'...' if len(paper_data['authors']) > 3 else ''}
ë°œí–‰ì¼: {paper_data['publication_date']}

ì´ˆë¡:
{paper_data['abstract']}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì—°êµ¬ëœ í™”í•™ ë¬¼ì§ˆ ë˜ëŠ” ì•½ë¬¼
2. ì£¼ìš” ì—°êµ¬ ë°©ë²•ë¡ 
3. í™”í•™ì /ì•½ë¦¬í•™ì  ë©”ì»¤ë‹ˆì¦˜
4. ì£¼ìš” ë°œê²¬ì‚¬í•­
5. ì„ìƒì  ì˜ì˜
6. í•œê³„ì  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥

ë¶„ì„ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ë¶„ì„ ìˆ˜í–‰
        analysis_result = llm.invoke(analysis_prompt)
        
        # HTML í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì •ë¦¬
        html_result = f"""
<div class="paper-analysis">
<h3>ğŸ“‹ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼</h3>
<div style="background-color: #f8f9fa; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;">
<h4 style="color: #1f2937; margin-bottom: 0.5rem;">ë…¼ë¬¸ ì •ë³´</h4>
<p><strong>ì œëª©:</strong> {paper_data['title']}</p>
<p><strong>ì €ì:</strong> {', '.join(paper_data['authors'][:3])}{'...' if len(paper_data['authors']) > 3 else ''}</p>
<p><strong>ë°œí–‰ì¼:</strong> {paper_data['publication_date']}</p>
<p><strong>ë§í¬:</strong> <a href="{paper_url}" target="_blank" style="color: #10b981;">ì›ë¬¸ ë³´ê¸°</a></p>
</div>

<div style="background-color: #ffffff; padding: 1rem; border: 1px solid #e5e7eb; border-radius: 0.5rem;">
<h4 style="color: #1f2937; margin-bottom: 1rem;">ğŸ”¬ í™”í•™ì  ë¶„ì„</h4>
<div style="white-space: pre-wrap; line-height: 1.6;">{analysis_result.content}</div>
</div>
</div>
"""
        
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì™„ë£Œ")
        return html_result
        
    except Exception as e:
        error_msg = f"ë…¼ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ë…¼ë¬¸ ë¶„ì„ ì˜¤ë¥˜: {error_msg}")
        return f"""
<div class="paper-analysis">
<h3>ğŸ“‹ ë…¼ë¬¸ ë¶„ì„ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ì˜¬ë°”ë¥¸ PubMed URLì¸ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
</p>
</div>
"""

@tool
def analyze_chemical_research_trends(topic: str, max_papers: int = 10) -> str:
    """íŠ¹ì • í™”í•™ ì£¼ì œì— ëŒ€í•œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ì™„ì „í•œ HTML í‘œ í˜•ì‹ì˜ ë³´ê³ ì„œë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ë°˜í™˜ëœ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ìš”ì•½ ì—†ì´ HTML ê²°ê³¼ë§Œ í‘œì‹œí•˜ì„¸ìš”."""
    try:
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë¶„ì„ ì‹œì‘: {topic}")
        
        # ë…¼ë¬¸ ê²€ìƒ‰ ìˆ˜í–‰
        import asyncio
        
        async def _search_papers_for_trends():
            searcher = paper_search(max_articles=max_papers)
            print(f"[DEBUG] paper_search ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨ (ë™í–¥ ë¶„ì„ìš©)")
            
            # í•„í„°ë¥¼ ì™„í™”í•œ ê²€ìƒ‰ URL (ëª¨ë“  ë…¼ë¬¸ íƒ€ì… í¬í•¨, ìµœê·¼ 5ë…„)
            await searcher.setup_browser()
            search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={topic}&filter=datesearch.y_5"
            
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ê²€ìƒ‰ URL: {search_url}")
            
            # 403 ì˜¤ë¥˜ ëŒ€ì‘ì„ ìœ„í•œ ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            articles = []
            
            for attempt in range(max_retries):
                page = await searcher.context.new_page()
                try:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}/{max_retries}")
                    
                    # í˜ì´ì§€ ë¡œë”© ì „ ëŒ€ê¸°
                    await asyncio.sleep(2 * (attempt + 1))
                    
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                    await asyncio.sleep(5)
                    
                    print("[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                    
                    # í˜ì´ì§€ ì œëª© í™•ì¸
                    page_title = await page.title()
                    print(f"[DEBUG] í˜ì´ì§€ ì œëª©: {page_title}")
                    
                    # 403 ì˜¤ë¥˜ ê°ì§€
                    if "403" in page_title or "Forbidden" in page_title:
                        print(f"[DEBUG] 403 ì˜¤ë¥˜ ê°ì§€ë¨. ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1}")
                        if attempt < max_retries - 1:
                            await page.close()
                            continue
                        else:
                            print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. 403 ì˜¤ë¥˜ ì§€ì†ë¨.")
                            await page.close()
                            break
                    
                    # ì •ìƒì ì¸ í˜ì´ì§€ì¸ ê²½ìš° ë…¼ë¬¸ ì¶”ì¶œ
                    articles = await searcher.extract_articles(page, topic)
                    await page.close()
                    break
                    
                except Exception as e:
                    print(f"[DEBUG] ë™í–¥ ë¶„ì„ ì‹œë„ {attempt + 1} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    await page.close()
                    if attempt < max_retries - 1:
                        continue
                    else:
                        print("[DEBUG] ë™í–¥ ë¶„ì„ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
                        break

            results = []
            print(f"\n[DEBUG] ë™í–¥ ë¶„ì„ìš© ì´ˆë¡ ì¶”ì¶œ ì‹œì‘... {len(articles)}ê°œì˜ ë…¼ë¬¸ ì²˜ë¦¬")
            
            for i, article in enumerate(articles, 1):
                print(f"\n=== ë…¼ë¬¸ {i}/{len(articles)} ì´ˆë¡ ì¶”ì¶œ ===")
                abstract = await searcher.extract_abstract(article["url"])
                results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "abstract": abstract
                })

            await searcher.close_browser()
            return results
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        results = None
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("[DEBUG] ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘, ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰")
                import concurrent.futures
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        return new_loop.run_until_complete(_search_papers_for_trends())
                    finally:
                        new_loop.close()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=120)
            else:
                print("[DEBUG] ê¸°ì¡´ ë£¨í”„ì—ì„œ ì‹¤í–‰")
                results = loop.run_until_complete(_search_papers_for_trends())
        except RuntimeError as e:
            print(f"[DEBUG] RuntimeError ë°œìƒ: {e}, ìƒˆ ë£¨í”„ ìƒì„±")
            results = asyncio.run(_search_papers_for_trends())
        
        if not results:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            return f"""
<div class="research-trends">
<h3>ğŸ“Š '{topic}' ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ì¶©ë¶„í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë™í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> ë” ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš” (ì˜ˆ: "cancer treatment", "drug delivery" ë“±)
</p>
</div>
"""
        
        print(f"[DEBUG] ë™í–¥ ë¶„ì„ìš© HTML í…Œì´ë¸” ìƒì„± ì¤‘, {len(results)}ê°œ ë…¼ë¬¸")
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½ ìƒì„±
        papers_summaries = []
        for i, paper in enumerate(results, 1):
            print(f"[DEBUG] ë…¼ë¬¸ {i} ìš”ì•½ ìƒì„± ì¤‘...")
            paper_prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ 2-3ì¤„ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}

ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
            try:
                paper_summary_result = llm.invoke(paper_prompt)
                paper_summary = paper_summary_result.content.strip()
                
                # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(paper_summary) > 200:
                    paper_summary = paper_summary[:200] + "..."
                    
                papers_summaries.append(paper_summary)
                    
            except Exception as e:
                print(f"[DEBUG] ë…¼ë¬¸ {i} ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                fallback_summary = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
                papers_summaries.append(fallback_summary)
        
        # ì—°êµ¬ ë™í–¥ ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ìš”ì•½ ìƒì„±
        all_papers_info = ""
        for i, (paper, summary) in enumerate(zip(results, papers_summaries), 1):
            all_papers_info += f"""
ë…¼ë¬¸ {i}: {paper['title']}
ì£¼ìš” ë‚´ìš©: {summary}

"""
        
        trend_analysis_prompt = f"""
ë‹¤ìŒì€ '{topic}' ì£¼ì œë¡œ ê²€ìƒ‰ëœ {len(results)}ê°œì˜ ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤:

{all_papers_info}

ì´ ë…¼ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ì£¼ìš” ì—°êµ¬ ë™í–¥ (2-3ê°œì˜ í•µì‹¬ íŠ¸ë Œë“œ)
2. ì£¼ëª©í•  ë§Œí•œ ì—°êµ¬ ì„±ê³¼
3. í–¥í›„ ì—°êµ¬ ë°©í–¥ ì „ë§

ê° í•­ëª©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        try:
            trend_analysis_result = llm.invoke(trend_analysis_prompt)
            trend_analysis = trend_analysis_result.content.strip()
        except Exception as e:
            print(f"[DEBUG] ë™í–¥ ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            trend_analysis = "ë™í–¥ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = f"""
<div class="research-trends">
<h3>ğŸ“Š '{topic}' ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ</h3>
<table class="papers-table" style="margin-top: 1rem;">
<thead>
<tr>
<th style="width: 35%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 50%;">ì£¼ìš” ë‚´ìš©</th>
<th style="width: 15%;">ë…¼ë¬¸ ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # ê° ë…¼ë¬¸ ì •ë³´ë¥¼ í…Œì´ë¸” í–‰ìœ¼ë¡œ ì¶”ê°€
        for i, (paper, summary) in enumerate(zip(results, papers_summaries), 1):
            html_table += f"""
<tr>
<td class="paper-title">{paper['title']}</td>
<td class="paper-summary">{summary}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸ ë³´ê¸°</a></td>
</tr>
"""
        
        html_table += f"""
</tbody>
</table>

<div style="margin-top: 2rem; padding: 1.5rem; background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 0.5rem;">
<h4 style="color: #1f2937; margin-bottom: 1rem;">ğŸ“ˆ ì—°êµ¬ ë™í–¥ ë¶„ì„</h4>
<div style="white-space: pre-wrap; line-height: 1.6; color: #374151;">{trend_analysis}</div>
</div>

<div style="margin-top: 1rem; padding: 1rem; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 0.5rem;">
<p style="color: #166534; font-size: 0.9rem;">
ğŸ’¡ <strong>ì°¸ê³ ì‚¬í•­:</strong> ì´ ë³´ê³ ì„œëŠ” PubMedì—ì„œ ê²€ìƒ‰ëœ ìµœê·¼ 5ë…„ê°„ì˜ ë…¼ë¬¸ {len(results)}í¸ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ìˆ˜ì§‘ëœ ë°ì´í„°ì´ë¯€ë¡œ, ë” ì•ˆì •ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ì„œëŠ” "APIë¡œ {topic} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¥¼ ì´ìš©í•´ë³´ì„¸ìš”.
</p>
</div>
</div>
"""
        
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"ì—°êµ¬ ë™í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[DEBUG] ì—°êµ¬ ë™í–¥ ë¶„ì„ ì˜¤ë¥˜: {error_msg}")
        return f"""
<div class="research-trends">
<h3>ğŸ“Š ì—°êµ¬ ë™í–¥ ë¶„ì„ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}<br><br>
ğŸ’¡ <strong>ëŒ€ì•ˆ:</strong> "APIë¡œ {topic} ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ì¤˜"ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.
</p>
</div>
"""

# í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ê²€ìƒ‰ í•¨ìˆ˜
@tool  
def test_paper_search(query: str = "aspirin") -> str:
    """í…ŒìŠ¤íŠ¸ìš© ë…¼ë¬¸ ê²€ìƒ‰ - ê°„ë‹¨í•œ ê²€ìƒ‰ì–´ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        print(f"[TEST] í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        result = search_research_papers_sync(query, 3)  # 3ê°œë§Œ ê²€ìƒ‰
        print(f"[TEST] í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ")
        return result
    except Exception as e:
        return f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

@tool
def api_paper_search_tool(query: str) -> str:
    """NCBI E-utilities APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰ (403 ì˜¤ë¥˜ ë°©ì§€). HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”."""
    try:
        print(f"[API] APIë¥¼ í†µí•œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
        
        import requests
        import xml.etree.ElementTree as ET
        from urllib.parse import quote
        
        # Step 1: ê²€ìƒ‰í•˜ì—¬ ë…¼ë¬¸ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': 5,  # ê¸°ë³¸ê°’ìœ¼ë¡œ 5ê°œ ì„¤ì •
            'retmode': 'xml',
            'datetype': 'pdat',
            'reldate': 1825,  # ìµœê·¼ 5ë…„ (365 * 5)
            'sort': 'relevance'
        }
        
        print(f"[API] ê²€ìƒ‰ ìš”ì²­ ì¤‘...")
        search_response = requests.get(search_url, params=search_params, timeout=30)
        
        if search_response.status_code != 200:
            return f"API ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {search_response.status_code}"
        
        # XML íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ID ì¶”ì¶œ
        search_root = ET.fromstring(search_response.content)
        id_list = search_root.find('.//IdList')
        
        if id_list is None or len(id_list) == 0:
            return f"""
<div class="research-papers-table">
<h3>ğŸ“š API ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
'{query}'ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<br><br>
ğŸ’¡ <strong>íŒ:</strong> ë” ì¼ë°˜ì ì¸ ì˜ì–´ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš” (ì˜ˆ: "cancer", "treatment", "therapy")
</p>
</div>
"""
        
        paper_ids = [id_elem.text for id_elem in id_list.findall('Id')]
        print(f"[API] ë°œê²¬ëœ ë…¼ë¬¸ ID: {len(paper_ids)}ê°œ")
        
        # Step 2: ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        fetch_params = {
            'db': 'pubmed',
            'id': ','.join(paper_ids),
            'retmode': 'xml'
        }
        
        print(f"[API] ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìš”ì²­ ì¤‘...")
        fetch_response = requests.get(fetch_url, params=fetch_params, timeout=30)
        
        if fetch_response.status_code != 200:
            return f"API ìƒì„¸ ì •ë³´ ìš”ì²­ ì‹¤íŒ¨: HTTP {fetch_response.status_code}"
        
        # XML íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        fetch_root = ET.fromstring(fetch_response.content)
        articles = fetch_root.findall('.//PubmedArticle')
        
        if not articles:
            return f"ë…¼ë¬¸ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[API] ë…¼ë¬¸ ìƒì„¸ ì •ë³´ íŒŒì‹± ì¤‘: {len(articles)}ê°œ")
        
        # ë…¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ LLMìœ¼ë¡œ ìš”ì•½
        papers_data = []
        for i, article in enumerate(articles[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ ì²˜ë¦¬
            try:
                # ì œëª© ì¶”ì¶œ
                title_elem = article.find('.//ArticleTitle')
                title = title_elem.text if title_elem is not None and title_elem.text else "ì œëª© ì—†ìŒ"
                
                # ì´ˆë¡ ì¶”ì¶œ
                abstract_elem = article.find('.//Abstract/AbstractText')
                abstract = abstract_elem.text if abstract_elem is not None and abstract_elem.text else "ì´ˆë¡ ì—†ìŒ"
                
                # ë°œí–‰ì—°ë„ ì¶”ì¶œ
                year_elem = article.find('.//PubDate/Year')
                year = year_elem.text if year_elem is not None and year_elem.text else "ì—°ë„ ë¯¸ìƒ"
                
                # PMID ì¶”ì¶œ
                pmid_elem = article.find('.//PMID')
                pmid = pmid_elem.text if pmid_elem is not None and pmid_elem.text else ""
                pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "#"
                
                papers_data.append({
                    'title': title,
                    'abstract': abstract,
                    'year': year,
                    'url': pubmed_url
                })
                
            except Exception as e:
                print(f"[API] ë…¼ë¬¸ {i} íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                continue
        
        if not papers_data:
            return "ë…¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"[API] LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ìš”ì•½ ì‹œì‘...")
        
        # LLMì„ ì‚¬ìš©í•´ì„œ ê° ë…¼ë¬¸ì„ ìš”ì•½
        papers_info = ""
        for i, paper in enumerate(papers_data, 1):
            papers_info += f"""
ë…¼ë¬¸ {i}:
ì œëª©: {paper['title']}
ì´ˆë¡: {paper['abstract']}
ì—°ë„: {paper['year']}
URL: {paper['url']}

---
"""
        
        summary_prompt = f"""
ë‹¤ìŒ {len(papers_data)}ê°œì˜ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ê° ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ê° ë…¼ë¬¸ë§ˆë‹¤ 2-3ì¤„ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.

{papers_info}

ê° ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ë…¼ë¬¸1||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
ë…¼ë¬¸2||ì œëª©||ì£¼ìš”ë‚´ìš©ìš”ì•½
...

ì£¼ìš”ë‚´ìš©ìš”ì•½ì€ ì—°êµ¬ ëª©ì , ë°©ë²•, ì£¼ìš” ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ 2-3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        # LLM ìš”ì•½ ìˆ˜í–‰
        summary_result = llm.invoke(summary_prompt)
        summary_lines = summary_result.content.strip().split('\n')
        
        # HTML í‘œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
        html_table = f"""
<div class="research-papers-table">
<h3>ğŸ“š API ê²€ìƒ‰ ê²°ê³¼: {query}</h3>
<table class="papers-table">
<thead>
<tr>
<th style="width: 40%;">ë…¼ë¬¸ ì œëª©</th>
<th style="width: 45%;">ì£¼ìš” ë‚´ìš© (AI ìš”ì•½)</th>
<th style="width: 8%;">ì—°ë„</th>
<th style="width: 7%;">ë§í¬</th>
</tr>
</thead>
<tbody>
"""
        
        # ìš”ì•½ëœ ë‚´ìš©ê³¼ ì›ë³¸ ë…¼ë¬¸ ì •ë³´ë¥¼ ë§¤ì¹­
        for i, paper in enumerate(papers_data):
            # LLM ìš”ì•½ì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì˜ ìš”ì•½ ì°¾ê¸°
            paper_summary = "ìš”ì•½ ìƒì„± ì¤‘..."
            for line in summary_lines:
                if f"ë…¼ë¬¸{i+1}||" in line:
                    parts = line.split("||")
                    if len(parts) >= 3:
                        paper_summary = parts[2].strip()
                    break
            
            # ìš”ì•½ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆë¡ ì‚¬ìš©
            if paper_summary == "ìš”ì•½ ìƒì„± ì¤‘..." or not paper_summary:
                paper_summary = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
            
            html_table += f"""
<tr>
<td class="paper-title">{paper['title']}</td>
<td class="paper-summary">{paper_summary}</td>
<td>{paper['year']}</td>
<td><a href="{paper['url']}" target="_blank" class="paper-link">ì›ë¬¸</a></td>
</tr>
"""
        
        html_table += """
</tbody>
</table>
<div style="margin-top: 1rem; padding: 0.5rem; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 0.5rem;">
<p style="color: #166534; font-size: 0.9rem;">
âœ… <strong>API ê²€ìƒ‰ ì„±ê³µ:</strong> NCBI E-utilities APIë¥¼ í†µí•´ ì•ˆì •ì ìœ¼ë¡œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.
</p>
</div>
</div>
"""
        
        print(f"[API] HTML í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        # HTML ë§ˆì»¤ ì œê±°í•˜ê³  ìˆœìˆ˜ HTMLë§Œ ë°˜í™˜
        return html_table
        
    except Exception as e:
        error_msg = f"API ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[API] ì˜ˆì™¸ ë°œìƒ: {error_msg}")
        return f"""
<div class="research-papers-table">
<h3>ğŸ“š API ê²€ìƒ‰ ì˜¤ë¥˜</h3>
<p style="color: #dc2626; padding: 1rem; background-color: #fef2f2; border: 1px solid #fecaca; border-radius: 0.5rem;">
{error_msg}
</p>
</div>
"""

@tool
def get_pubmed_search_tips() -> str:
    """PubMed ê²€ìƒ‰ì´ ì˜ ì•ˆë  ë•Œ ë„ì›€ì´ ë˜ëŠ” íŒì„ ì œê³µí•©ë‹ˆë‹¤."""
    return """
<div class="search-tips">
<h3>ğŸ” PubMed ê²€ìƒ‰ ìµœì í™” íŒ</h3>

<div style="background-color: #f0fdf4; padding: 1rem; border: 1px solid #bbf7d0; border-radius: 0.5rem; margin: 1rem 0;">
<h4 style="color: #166534; margin-bottom: 0.5rem;">âœ… íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´</h4>
<ul style="color: #166534; margin-left: 1rem;">
<li><strong>ì¼ë°˜ì ì¸ ìš©ì–´ ì‚¬ìš©:</strong> "cancer", "treatment", "therapy"</li>
<li><strong>ì˜ì–´ ê²€ìƒ‰ì–´:</strong> í•œê¸€ë³´ë‹¤ ì˜ì–´ê°€ ë” ë§ì€ ê²°ê³¼</li>
<li><strong>MeSH ìš©ì–´:</strong> ì˜í•™ í‘œì¤€ ìš©ì–´ í™œìš©</li>
<li><strong>ë™ì˜ì–´ í¬í•¨:</strong> "drug OR medication OR pharmaceutical"</li>
</ul>
</div>

<div style="background-color: #fef2f2; padding: 1rem; border: 1px solid #fecaca; border-radius: 0.5rem; margin: 1rem 0;">
<h4 style="color: #dc2626; margin-bottom: 0.5rem;">âŒ í”¼í•´ì•¼ í•  ê²€ìƒ‰ì–´</h4>
<ul style="color: #dc2626; margin-left: 1rem;">
<li><strong>ë„ˆë¬´ êµ¬ì²´ì :</strong> "doxorubicin cardiotoxicity prevention mechanism"</li>
<li><strong>ë¸Œëœë“œëª…:</strong> ì¼ë°˜ëª… ì‚¬ìš© ê¶Œì¥</li>
<li><strong>ë³µì¡í•œ ì¡°í•©:</strong> 3ê°œ ì´ìƒì˜ AND ì¡°ê±´</li>
</ul>
</div>

<div style="background-color: #f8f9fa; padding: 1rem; border: 1px solid #e9ecef; border-radius: 0.5rem;">
<h4 style="color: #495057; margin-bottom: 0.5rem;">ğŸ¯ ì¶”ì²œ ê²€ìƒ‰ì–´ ì˜ˆì‹œ</h4>
<ul style="color: #495057; margin-left: 1rem;">
<li>"cancer treatment"</li>
<li>"immunotherapy"</li>
<li>"drug delivery"</li>
<li>"chemotherapy"</li>
<li>"biomarker"</li>
<li>"clinical trial"</li>
</ul>
</div>
</div>
"""

tools = [
    generate_molecule_image, 
    name_to_smiles_cir, 
    paper_search_tool,  # ë‹¨ìˆœí™”ëœ ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬
    api_paper_search_tool,  # ë‹¨ìˆœí™”ëœ API ê²€ìƒ‰ ë„êµ¬
    analyze_chemical_paper,
    analyze_chemical_research_trends,
    get_pubmed_search_tips
]

# LangGraph ì„¤ì •
class State(TypedDict):
    messages: Annotated[list, add_messages]

class BasicToolNode:
    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result) if not isinstance(tool_result, str) else tool_result,
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}

# ê·¸ë˜í”„ êµ¬ì„±
graph_builder = StateGraph(State)
tool_node = BasicToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)

llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

def route_tools(state: State):
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END

graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    {"tools": "tools", END: END},
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatMessage(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    image: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def get_chat_interface():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except Exception as e:
        return HTMLResponse(f"<h1>Error loading page: {str(e)}</h1>")

@app.post("/chat")
async def chat_endpoint(chat_message: ChatMessage):
    try:
        # HJ ChemAgent ì‹¤í–‰
        result = graph.invoke({"messages": [{"role": "user", "content": chat_message.message}]})
        
        # HTML ë„êµ¬ ê²°ê³¼ ì§ì ‘ ê°ì§€ ë° ìš°íšŒ ì²˜ë¦¬
        html_tool_result = None
        for message in result["messages"]:
            if hasattr(message, 'name') and message.name in ['api_paper_search_tool', 'paper_search_tool', 'analyze_chemical_research_trends']:
                tool_content = message.content
                if isinstance(tool_content, str) and (
                    '<div class="research-papers-table">' in tool_content or 
                    '<div class="research-trends">' in tool_content or
                    '<div class="paper-analysis">' in tool_content or
                    '<table' in tool_content
                ):
                    html_tool_result = tool_content
                    print(f"[DEBUG] HTML ë„êµ¬ ê²°ê³¼ ì§ì ‘ ê°ì§€ë¨ - ë„êµ¬: {message.name}, ê¸¸ì´: {len(tool_content)}")
                    break
        
        # HTML ë„êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ LLM ì‘ë‹µ ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©
        if html_tool_result:
            response_text = html_tool_result
            print(f"[DEBUG] LLM ìš°íšŒí•˜ì—¬ ë„êµ¬ ê²°ê³¼ ì§ì ‘ ì‚¬ìš©")
        else:
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            last_message = result["messages"][-1]
            response_text = last_message.content
        
        # HTML í…Œì´ë¸” ì§ì ‘ ê°ì§€ ë° ì²˜ë¦¬ (ë°±ì—”ë“œì—ì„œ ì¶”ê°€ ë³´ì¥)
        print(f"[DEBUG] ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
        print(f"[DEBUG] ì‘ë‹µ ì‹œì‘ 100ì: {response_text[:100]}")
        
        # HTML íƒœê·¸ ê°ì§€
        has_html = ("<div" in response_text and "</div>" in response_text) or ("<table" in response_text and "</table>" in response_text)
        
        if has_html:
            print(f"[DEBUG] HTML íƒœê·¸ ê°ì§€ë¨")
            
            # HTML ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
            html_start = response_text.find("<div")
            if html_start == -1:
                html_start = response_text.find("<table")
            
            if html_start > 0:
                # HTML ì•ì— ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì œê±°
                original_length = len(response_text)
                response_text = response_text[html_start:].strip()
                print(f"[DEBUG] HTML ì•ì˜ í…ìŠ¤íŠ¸ ì œê±°ë¨ - ì›ë˜ ê¸¸ì´: {original_length}, ìƒˆ ê¸¸ì´: {len(response_text)}")
            
            # ì¶”ê°€ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ HTMLì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if any(phrase in response_text for phrase in ["ê²°ê³¼ì…ë‹ˆë‹¤", "ë¶„ì„í–ˆìŠµë‹ˆë‹¤", "ë³´ê³ ì„œì…ë‹ˆë‹¤", "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤"]):
                print(f"[DEBUG] ì¶”ê°€ ì„¤ëª… í…ìŠ¤íŠ¸ ê°ì§€ë¨, HTML ì¶”ì¶œ ì‹œë„")
                import re
                
                # ë” í¬ê´„ì ì¸ HTML ë§¤ì¹­
                html_patterns = [
                    r'<div[^>]*class="research-trends"[^>]*>.*?</div>',
                    r'<div[^>]*class="research-papers-table"[^>]*>.*?</div>',
                    r'<div[^>]*class="paper-analysis"[^>]*>.*?</div>',
                    r'<table[^>]*>.*?</table>'
                ]
                
                for pattern in html_patterns:
                    html_match = re.search(pattern, response_text, re.DOTALL)
                    if html_match:
                        extracted_html = html_match.group(0)
                        print(f"[DEBUG] íŒ¨í„´ '{pattern[:30]}...'ë¡œ HTML ì¶”ì¶œë¨ - ê¸¸ì´: {len(extracted_html)}")
                        response_text = extracted_html
                        break
                else:
                    print(f"[DEBUG] HTML íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨, ì›ë³¸ ìœ ì§€")
        else:
            print(f"[DEBUG] HTML íƒœê·¸ ì—†ìŒ, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
        
        print(f"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
        print(f"[DEBUG] ìµœì¢… ì‘ë‹µ ì‹œì‘ 200ì: {response_text[:200]}")
        
        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì‘ë‹µì¸ì§€ í™•ì¸
        image_data = None
        for message in result["messages"]:
            if hasattr(message, 'content') and isinstance(message.content, str):
                # JSONìœ¼ë¡œ ì¸ì½”ë”©ëœ íˆ´ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                try:
                    import json
                    content = json.loads(message.content)
                    if isinstance(content, str) and content.startswith("data:image/png;base64,"):
                        image_data = content
                        break
                except:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ í™•ì¸
                    if message.content.startswith("data:image/png;base64,"):
                        image_data = message.content
                        break
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ base64 ì´ë¯¸ì§€ ë°ì´í„° ì œê±° (í…ìŠ¤íŠ¸ë¡œ í‘œì‹œë˜ì§€ ì•Šë„ë¡)
        if image_data and image_data in response_text:
            response_text = response_text.replace(image_data, "").strip()
        
        # base64 ë°ì´í„°ê°€ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì œê±°í•˜ê³  ê°„ë‹¨í•œ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
        if "data:image/png;base64," in response_text:
            response_text = "ë¶„ìì˜ êµ¬ì¡° ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        
        return ChatResponse(response=response_text, image=image_data)
        
    except Exception as e:
        return ChatResponse(
            response=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ì •ì  íŒŒì¼ ì„œë¹™ (í•„ìš”ì‹œ)
# try:
#     app.mount("/static", StaticFiles(directory="static"), name="static")
# except Exception as e:
#     print(f"Static files mounting error: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8002)
